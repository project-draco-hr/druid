{
  List<HadoopyShardSpec> specs=Lists.newArrayList();
  final int partitionCount=10;
  for (int i=0; i < partitionCount; i++) {
    specs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,partitionCount,null,new DefaultObjectMapper()),i));
  }
  HadoopIngestionSpec spec=new HadoopIngestionSpec(new DataSchema("foo",null,new AggregatorFactory[0],new UniformGranularitySpec(Granularity.MINUTE,QueryGranularity.MINUTE,ImmutableList.of(new Interval("2010-01-01/P1D"))),jsonMapper),new HadoopIOConfig(ImmutableMap.<String,Object>of("paths","bar","type","static"),null,null),new HadoopTuningConfig(null,null,null,ImmutableMap.of(new DateTime("2010-01-01T01:00:00"),specs),null,null,false,false,false,false,null,false,false,null,null,null));
  HadoopDruidIndexerConfig config=HadoopDruidIndexerConfig.fromSpec(spec);
  final List<String> dims=Arrays.asList("diM1","dIM2");
  final ImmutableMap<String,Object> values=ImmutableMap.<String,Object>of("Dim1","1","DiM2","2","dim1","3","dim2","4");
  final long timestamp=new DateTime("2010-01-01T01:00:01").getMillis();
  final Bucket expectedBucket=config.getBucket(new MapBasedInputRow(timestamp,dims,values)).get();
  final long nextBucketTimestamp=QueryGranularity.MINUTE.next(QueryGranularity.MINUTE.truncate(timestamp));
  for (int i=0; timestamp + i < nextBucketTimestamp; i++) {
    Assert.assertEquals(expectedBucket.partitionNum,config.getBucket(new MapBasedInputRow(timestamp + i,dims,values)).get().partitionNum);
  }
}
