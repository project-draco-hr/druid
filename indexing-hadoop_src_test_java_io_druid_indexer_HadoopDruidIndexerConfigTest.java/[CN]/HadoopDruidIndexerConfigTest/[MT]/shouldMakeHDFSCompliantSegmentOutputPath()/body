{
  HadoopIngestionSpec schema;
  try {
    schema=jsonReadWriteRead("{\n" + "    \"dataSchema\": {\n" + "        \"dataSource\": \"source\",\n"+ "        \"metricsSpec\": [],\n"+ "        \"granularitySpec\": {\n"+ "            \"type\": \"uniform\",\n"+ "            \"segmentGranularity\": \"hour\",\n"+ "            \"intervals\": [\"2012-07-10/P1D\"]\n"+ "        }\n"+ "    },\n"+ "    \"ioConfig\": {\n"+ "        \"type\": \"hadoop\",\n"+ "        \"segmentOutputPath\": \"hdfs://server:9100/tmp/druid/datatest\"\n"+ "    }\n"+ "}",HadoopIngestionSpec.class);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  HadoopDruidIndexerConfig cfg=new HadoopDruidIndexerConfig(schema.withTuningConfig(schema.getTuningConfig().withVersion("some:brand:new:version")));
  Bucket bucket=new Bucket(4711,new DateTime(2012,07,10,5,30),4712);
  Path path=JobHelper.makeSegmentOutputPath(new Path(cfg.getSchema().getIOConfig().getSegmentOutputPath()),new DistributedFileSystem(),cfg.getSchema().getDataSchema().getDataSource(),cfg.getSchema().getTuningConfig().getVersion(),cfg.getSchema().getDataSchema().getGranularitySpec().bucketInterval(bucket.time).get(),bucket.partitionNum);
  Assert.assertEquals("hdfs://server:9100/tmp/druid/datatest/source/20120710T050000.000Z_20120710T060000.000Z/some_brand_new_version/4712",path.toString());
}
