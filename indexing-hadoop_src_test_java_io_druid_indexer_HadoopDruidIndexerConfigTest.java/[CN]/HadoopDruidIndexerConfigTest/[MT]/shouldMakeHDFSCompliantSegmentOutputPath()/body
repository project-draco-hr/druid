{
  HadoopIngestionSpec schema;
  try {
    schema=jsonReadWriteRead("{" + "\"dataSource\": \"source\"," + " \"granularitySpec\":{"+ "   \"type\":\"uniform\","+ "   \"gran\":\"hour\","+ "   \"intervals\":[\"2012-07-10/P1D\"]"+ " },"+ "\"segmentOutputPath\": \"hdfs://server:9100/tmp/druid/datatest\""+ "}",HadoopIngestionSpec.class);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  HadoopDruidIndexerConfig cfg=new HadoopDruidIndexerConfig(schema.withTuningConfig(schema.getTuningConfig().withVersion("some:brand:new:version")));
  Bucket bucket=new Bucket(4711,new DateTime(2012,07,10,5,30),4712);
  Path path=cfg.makeSegmentOutputPath(new DistributedFileSystem(),bucket);
  Assert.assertEquals("hdfs://server:9100/tmp/druid/datatest/source/20120710T050000.000Z_20120710T060000.000Z/some_brand_new_version/4712",path.toString());
}
