{
  HadoopIngestionSpec spec=new HadoopIngestionSpec(new DataSchema("foo",null,new AggregatorFactory[0],new UniformGranularitySpec(Granularity.MINUTE,QueryGranularities.MINUTE,ImmutableList.of(new Interval("2010-01-01/P1D"))),jsonMapper),new HadoopIOConfig(ImmutableMap.<String,Object>of("paths","bar","type","static"),null,null),new HadoopTuningConfig(null,null,null,ImmutableMap.<DateTime,List<HadoopyShardSpec>>of(new DateTime("2010-01-01T01:00:00"),Lists.newArrayList(new HadoopyShardSpec(NoneShardSpec.instance(),1)),new DateTime("2010-01-01T02:00:00"),Lists.newArrayList(new HadoopyShardSpec(NoneShardSpec.instance(),2))),null,null,false,false,false,false,null,false,false,null,null,null,false));
  HadoopDruidIndexerConfig config=HadoopDruidIndexerConfig.fromSpec(spec);
  final List<String> dims=Arrays.asList("diM1","dIM2");
  final ImmutableMap<String,Object> values=ImmutableMap.<String,Object>of("Dim1","1","DiM2","2","dim1","3","dim2","4");
  final long ts1=new DateTime("2010-01-01T01:00:01").getMillis();
  Assert.assertEquals(config.getBucket(new MapBasedInputRow(ts1,dims,values)).get().getShardNum(),1);
  final long ts2=new DateTime("2010-01-01T02:00:01").getMillis();
  Assert.assertEquals(config.getBucket(new MapBasedInputRow(ts2,dims,values)).get().getShardNum(),2);
}
