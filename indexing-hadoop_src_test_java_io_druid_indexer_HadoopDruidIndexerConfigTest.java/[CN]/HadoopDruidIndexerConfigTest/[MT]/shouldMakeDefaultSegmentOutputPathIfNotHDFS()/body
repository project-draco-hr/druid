{
  final HadoopIngestionSpec schema;
  try {
    schema=jsonReadWriteRead("{\n" + "    \"dataSchema\": {\n" + "        \"dataSource\": \"the:data:source\",\n"+ "        \"metricsSpec\": [],\n"+ "        \"granularitySpec\": {\n"+ "            \"type\": \"uniform\",\n"+ "            \"segmentGranularity\": \"hour\",\n"+ "            \"intervals\": [\"2012-07-10/P1D\"]\n"+ "        }\n"+ "    },\n"+ "    \"ioConfig\": {\n"+ "        \"type\": \"hadoop\",\n"+ "        \"segmentOutputPath\": \"/tmp/dru:id/data:test\"\n"+ "    }\n"+ "}",HadoopIngestionSpec.class);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  HadoopDruidIndexerConfig cfg=new HadoopDruidIndexerConfig(schema.withTuningConfig(schema.getTuningConfig().withVersion("some:brand:new:version")));
  Bucket bucket=new Bucket(4711,new DateTime(2012,07,10,5,30),4712);
  Path path=JobHelper.makeSegmentOutputPath(new Path(cfg.getSchema().getIOConfig().getSegmentOutputPath()),new LocalFileSystem(),cfg.getSchema().getDataSchema().getDataSource(),cfg.getSchema().getTuningConfig().getVersion(),cfg.getSchema().getDataSchema().getGranularitySpec().bucketInterval(bucket.time).get(),bucket.partitionNum);
  Assert.assertEquals("file:/tmp/dru:id/data:test/the:data:source/2012-07-10T05:00:00.000Z_2012-07-10T06:00:00.000Z/some:brand:new:version/4712",path.toString());
}
