{
  SortableBytes keyBytes=SortableBytes.fromBytesWritable(key);
  Bucket bucket=Bucket.fromGroupKey(keyBytes.getGroupKey()).lhs;
  final Interval interval=config.getGranularitySpec().bucketInterval(bucket.time).get();
  ListeningExecutorService persistExecutor=null;
  List<ListenableFuture<?>> persistFutures=Lists.newArrayList();
  IncrementalIndex index=makeIncrementalIndex(bucket,combiningAggs,config,null,null);
  try {
    File baseFlushFile=File.createTempFile("base","flush");
    baseFlushFile.delete();
    baseFlushFile.mkdirs();
    Set<File> toMerge=Sets.newTreeSet();
    int indexCount=0;
    int lineCount=0;
    int runningTotalLineCount=0;
    long startTime=System.currentTimeMillis();
    Set<String> allDimensionNames=Sets.newLinkedHashSet();
    final ProgressIndicator progressIndicator=makeProgressIndicator(context);
    int numBackgroundPersistThreads=config.getSchema().getTuningConfig().getNumBackgroundPersistThreads();
    if (numBackgroundPersistThreads > 0) {
      final BlockingQueue<Runnable> queue=new SynchronousQueue<>();
      ExecutorService executorService=new ThreadPoolExecutor(numBackgroundPersistThreads,numBackgroundPersistThreads,0L,TimeUnit.MILLISECONDS,queue,Execs.makeThreadFactory("IndexGeneratorJob_persist_%d"),new RejectedExecutionHandler(){
        @Override public void rejectedExecution(        Runnable r,        ThreadPoolExecutor executor){
          try {
            executor.getQueue().put(r);
          }
 catch (          InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new RejectedExecutionException("Got Interrupted while adding to the Queue");
          }
        }
      }
);
      persistExecutor=MoreExecutors.listeningDecorator(executorService);
    }
 else {
      persistExecutor=MoreExecutors.sameThreadExecutor();
    }
    for (    final BytesWritable bw : values) {
      context.progress();
      final InputRow inputRow=index.formatRow(InputRowSerde.fromBytes(bw.getBytes(),aggregators));
      int numRows=index.add(inputRow);
      ++lineCount;
      if (!index.canAppendRow()) {
        allDimensionNames.addAll(index.getDimensionOrder());
        log.info(index.getOutOfRowsReason());
        log.info("%,d lines to %,d rows in %,d millis",lineCount - runningTotalLineCount,numRows,System.currentTimeMillis() - startTime);
        runningTotalLineCount=lineCount;
        final File file=new File(baseFlushFile,String.format("index%,05d",indexCount));
        toMerge.add(file);
        context.progress();
        final IncrementalIndex persistIndex=index;
        persistFutures.add(persistExecutor.submit(new ThreadRenamingRunnable(String.format("%s-persist",file.getName())){
          @Override public void doRun(){
            try {
              persist(persistIndex,interval,file,progressIndicator);
            }
 catch (            Exception e) {
              log.error(e,"persist index error");
              throw Throwables.propagate(e);
            }
 finally {
              persistIndex.close();
            }
          }
        }
));
        index=makeIncrementalIndex(bucket,combiningAggs,config,allDimensionNames,persistIndex.getColumnCapabilities());
        startTime=System.currentTimeMillis();
        ++indexCount;
      }
    }
    allDimensionNames.addAll(index.getDimensionOrder());
    log.info("%,d lines completed.",lineCount);
    List<QueryableIndex> indexes=Lists.newArrayListWithCapacity(indexCount);
    final File mergedBase;
    if (toMerge.size() == 0) {
      if (index.isEmpty()) {
        throw new IAE("If you try to persist empty indexes you are going to have a bad time");
      }
      mergedBase=new File(baseFlushFile,"merged");
      persist(index,interval,mergedBase,progressIndicator);
    }
 else {
      if (!index.isEmpty()) {
        final File finalFile=new File(baseFlushFile,"final");
        persist(index,interval,finalFile,progressIndicator);
        toMerge.add(finalFile);
      }
      Futures.allAsList(persistFutures).get(1,TimeUnit.HOURS);
      persistExecutor.shutdown();
      for (      File file : toMerge) {
        indexes.add(HadoopDruidIndexerConfig.INDEX_IO.loadIndex(file));
      }
      mergedBase=mergeQueryableIndex(indexes,aggregators,new File(baseFlushFile,"merged"),progressIndicator);
    }
    final FileSystem outputFS=new Path(config.getSchema().getIOConfig().getSegmentOutputPath()).getFileSystem(context.getConfiguration());
    final DataSegment segmentTemplate=new DataSegment(config.getDataSource(),interval,config.getSchema().getTuningConfig().getVersion(),null,ImmutableList.copyOf(allDimensionNames),metricNames,config.getShardSpec(bucket).getActualSpec(),-1,-1);
    final DataSegment segment=JobHelper.serializeOutIndex(segmentTemplate,context.getConfiguration(),context,context.getTaskAttemptID(),mergedBase,JobHelper.makeSegmentOutputPath(new Path(config.getSchema().getIOConfig().getSegmentOutputPath()),outputFS,segmentTemplate));
    Path descriptorPath=config.makeDescriptorInfoPath(segment);
    descriptorPath=JobHelper.prependFSIfNullScheme(FileSystem.get(descriptorPath.toUri(),context.getConfiguration()),descriptorPath);
    log.info("Writing descriptor to path[%s]",descriptorPath);
    JobHelper.writeSegmentDescriptor(config.makeDescriptorInfoDir().getFileSystem(context.getConfiguration()),segment,descriptorPath,context);
    for (    File file : toMerge) {
      FileUtils.deleteDirectory(file);
    }
  }
 catch (  ExecutionException e) {
    throw Throwables.propagate(e);
  }
catch (  TimeoutException e) {
    throw Throwables.propagate(e);
  }
 finally {
    index.close();
    if (persistExecutor != null) {
      persistExecutor.shutdownNow();
    }
  }
}
