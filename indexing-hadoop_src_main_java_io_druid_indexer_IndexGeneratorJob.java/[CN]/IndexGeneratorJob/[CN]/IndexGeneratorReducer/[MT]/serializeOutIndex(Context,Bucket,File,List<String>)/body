{
  Interval interval=config.getGranularitySpec().bucketInterval(bucket.time).get();
  int attemptNumber=context.getTaskAttemptID().getId();
  final FileSystem intermediateFS=config.makeDescriptorInfoDir().getFileSystem(context.getConfiguration());
  final FileSystem outputFS=new Path(config.getSchema().getIOConfig().getSegmentOutputPath()).getFileSystem(context.getConfiguration());
  final Path indexBasePath=config.makeSegmentOutputPath(outputFS,bucket);
  final Path indexZipFilePath=new Path(indexBasePath,String.format("index.zip.%s",attemptNumber));
  outputFS.mkdirs(indexBasePath);
  Exception caughtException=null;
  ZipOutputStream out=null;
  long size=0;
  try {
    out=new ZipOutputStream(new BufferedOutputStream(outputFS.create(indexZipFilePath),256 * 1024));
    List<String> filesToCopy=Arrays.asList(mergedBase.list());
    for (    String file : filesToCopy) {
      size+=copyFile(context,out,mergedBase,file);
    }
  }
 catch (  Exception e) {
    caughtException=e;
  }
 finally {
    if (caughtException == null) {
      Closeables.close(out,false);
    }
 else {
      CloseQuietly.close(out);
      throw Throwables.propagate(caughtException);
    }
  }
  Path finalIndexZipFilePath=new Path(indexBasePath,"index.zip");
  final URI indexOutURI=finalIndexZipFilePath.toUri();
  ImmutableMap<String,Object> loadSpec;
  String fsClazz=outputFS.getClass().getName();
  if ("org.apache.hadoop.fs.s3native.NativeS3FileSystem".equals(fsClazz)) {
    loadSpec=ImmutableMap.<String,Object>of("type","s3_zip","bucket",indexOutURI.getHost(),"key",indexOutURI.getPath().substring(1));
  }
 else   if ("org.apache.hadoop.fs.LocalFileSystem".equals(fsClazz)) {
    loadSpec=ImmutableMap.<String,Object>of("type","local","path",indexOutURI.getPath());
  }
 else   if ("org.apache.hadoop.hdfs.DistributedFileSystem".equals(fsClazz)) {
    loadSpec=ImmutableMap.<String,Object>of("type","hdfs","path",indexOutURI.toString());
  }
 else {
    throw new ISE("Unknown file system[%s]",fsClazz);
  }
  DataSegment segment=new DataSegment(config.getDataSource(),interval,config.getSchema().getTuningConfig().getVersion(),loadSpec,dimensionNames,metricNames,config.getShardSpec(bucket).getActualSpec(),SegmentUtils.getVersionFromDir(mergedBase),size);
  boolean success=false;
  for (int i=0; i < 6; i++) {
    if (renameIndexFiles(intermediateFS,outputFS,indexBasePath,indexZipFilePath,finalIndexZipFilePath,segment)) {
      log.info("Successfully renamed [%s] to [%s]",indexZipFilePath,finalIndexZipFilePath);
      success=true;
      break;
    }
 else {
      log.info("Failed to rename [%s] to [%s]",indexZipFilePath,finalIndexZipFilePath);
      try {
        Thread.sleep(10000);
        context.progress();
      }
 catch (      InterruptedException e) {
        throw new ISE("Thread error in retry loop for renaming [%s] to [%s]",indexZipFilePath.toUri().getPath(),finalIndexZipFilePath.toUri().getPath());
      }
    }
  }
  if (!success) {
    if (!outputFS.exists(indexZipFilePath)) {
      throw new ISE("File [%s] does not exist after retry loop.",indexZipFilePath.toUri().getPath());
    }
    if (outputFS.getFileStatus(indexZipFilePath).getLen() == outputFS.getFileStatus(finalIndexZipFilePath).getLen()) {
      outputFS.delete(indexZipFilePath,true);
    }
 else {
      outputFS.delete(finalIndexZipFilePath,true);
      if (!renameIndexFiles(intermediateFS,outputFS,indexBasePath,indexZipFilePath,finalIndexZipFilePath,segment)) {
        throw new ISE("Files [%s] and [%s] are different, but still cannot rename after retry loop",indexZipFilePath.toUri().getPath(),finalIndexZipFilePath.toUri().getPath());
      }
    }
  }
}
