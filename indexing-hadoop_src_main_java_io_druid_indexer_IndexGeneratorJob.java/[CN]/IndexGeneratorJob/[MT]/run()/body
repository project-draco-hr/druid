{
  try {
    Job job=new Job(new Configuration(),String.format("%s-index-generator-%s",config.getDataSource(),config.getIntervals()));
    job.getConfiguration().set("io.sort.record.percent","0.23");
    JobHelper.injectSystemProperties(job);
    job.setInputFormatClass(TextInputFormat.class);
    job.setMapperClass(IndexGeneratorMapper.class);
    job.setMapOutputValueClass(Text.class);
    SortableBytes.useSortableBytesAsMapOutputKey(job);
    job.setNumReduceTasks(Iterables.size(config.getAllBuckets().get()));
    job.setPartitionerClass(IndexGeneratorPartitioner.class);
    job.setReducerClass(IndexGeneratorReducer.class);
    job.setOutputKeyClass(BytesWritable.class);
    job.setOutputValueClass(Text.class);
    job.setOutputFormatClass(IndexGeneratorOutputFormat.class);
    FileOutputFormat.setOutputPath(job,config.makeIntermediatePath());
    config.addInputPaths(job);
    config.addJobProperties(job);
    config.intoConfiguration(job);
    JobHelper.setupClasspath(config,job);
    job.submit();
    log.info("Job %s submitted, status available at %s",job.getJobName(),job.getTrackingURL());
    boolean success=job.waitForCompletion(true);
    Counter invalidRowCount=job.getCounters().findCounter(HadoopDruidIndexerConfig.IndexJobCounters.INVALID_ROW_COUNTER);
    jobStats.setInvalidRowCount(invalidRowCount.getValue());
    return success;
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}
