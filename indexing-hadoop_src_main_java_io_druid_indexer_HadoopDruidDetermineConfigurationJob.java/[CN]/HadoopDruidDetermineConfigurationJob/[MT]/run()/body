{
  List<Jobby> jobs=Lists.newArrayList();
  JobHelper.ensurePaths(config);
  if (config.isDeterminingPartitions()) {
    jobs.add(config.getPartitionsSpec().getPartitionJob(config));
  }
 else {
    int shardsPerInterval=config.getPartitionsSpec().getNumShards();
    Map<DateTime,List<HadoopyShardSpec>> shardSpecs=Maps.newTreeMap(DateTimeComparator.getInstance());
    int shardCount=0;
    for (    Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {
      DateTime bucket=segmentGranularity.getStart();
      if (shardsPerInterval > 0) {
        List<HadoopyShardSpec> specs=Lists.newArrayListWithCapacity(shardsPerInterval);
        for (int i=0; i < shardsPerInterval; i++) {
          specs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,shardsPerInterval,config.getPartitionsSpec().getPartitionDimensions(),HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));
        }
        shardSpecs.put(bucket,specs);
        log.info("DateTime[%s], spec[%s]",bucket,specs);
      }
 else {
        final HadoopyShardSpec spec=new HadoopyShardSpec(new NoneShardSpec(),shardCount++);
        shardSpecs.put(bucket,Lists.newArrayList(spec));
        log.info("DateTime[%s], spec[%s]",bucket,spec);
      }
    }
    config.setShardSpecs(shardSpecs);
  }
  return JobHelper.runJobs(jobs,config);
}
