{
  final File tmpDir=new File(toolbox.getTaskWorkDir(),String.format("%s_%s_%s_%s_%s",this.getDataSource(),interval.getStart(),interval.getEnd(),version,shardSpec.getPartitionNum()));
  final FirehoseFactory firehoseFactory=ingestionSchema.getIOConfig().getFirehoseFactory();
  final int rowFlushBoundary=ingestionSchema.getTuningConfig().getRowFlushBoundary();
  final List<DataSegment> pushedSegments=new CopyOnWriteArrayList<DataSegment>();
  final DataSegmentPusher wrappedDataSegmentPusher=new DataSegmentPusher(){
    @Override public String getPathForHadoop(    String dataSource){
      return toolbox.getSegmentPusher().getPathForHadoop(dataSource);
    }
    @Override public DataSegment push(    File file,    DataSegment segment) throws IOException {
      final DataSegment pushedSegment=toolbox.getSegmentPusher().push(file,segment);
      pushedSegments.add(pushedSegment);
      return pushedSegment;
    }
  }
;
  final FireDepartmentMetrics metrics=new FireDepartmentMetrics();
  final Firehose firehose=firehoseFactory.connect(ingestionSchema.getDataSchema().getParser());
  final Plumber plumber=new YeOldePlumberSchool(interval,version,wrappedDataSegmentPusher,tmpDir).findPlumber(schema,convertTuningConfig(shardSpec,ingestionSchema.getTuningConfig()),metrics);
  final int myRowFlushBoundary=rowFlushBoundary > 0 ? rowFlushBoundary : toolbox.getConfig().getDefaultRowFlushBoundary();
  final QueryGranularity rollupGran=ingestionSchema.getDataSchema().getGranularitySpec().getQueryGranularity();
  try {
    plumber.startJob();
    while (firehose.hasMore()) {
      final InputRow inputRow=firehose.nextRow();
      if (shouldIndex(shardSpec,interval,inputRow,rollupGran)) {
        int numRows=plumber.add(inputRow);
        if (numRows == -1) {
          throw new ISE(String.format("Was expecting non-null sink for timestamp[%s]",new DateTime(inputRow.getTimestampFromEpoch())));
        }
        metrics.incrementProcessed();
        if (numRows >= myRowFlushBoundary) {
          plumber.persist(firehose.commit());
        }
      }
 else {
        metrics.incrementThrownAway();
      }
    }
  }
  finally {
    firehose.close();
  }
  plumber.persist(firehose.commit());
  try {
    plumber.finishJob();
  }
  finally {
    log.info("Task[%s] interval[%s] partition[%d] took in %,d rows (%,d processed, %,d unparseable, %,d thrown away)" + " and output %,d rows",getId(),interval,shardSpec.getPartitionNum(),metrics.processed() + metrics.unparseable() + metrics.thrownAway(),metrics.processed(),metrics.unparseable(),metrics.thrownAway(),metrics.rowOutput());
  }
  return Iterables.getOnlyElement(pushedSegments);
}
