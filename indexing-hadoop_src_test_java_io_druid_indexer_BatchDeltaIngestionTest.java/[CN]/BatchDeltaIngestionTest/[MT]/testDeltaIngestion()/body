{
  File tmpDir=temporaryFolder.newFolder();
  File dataFile1=new File(tmpDir,"data1");
  FileUtils.writeLines(dataFile1,ImmutableList.of("2014102200,a.example.com,a.example.com,90","2014102201,b.example.com,b.example.com,25"));
  File dataFile2=new File(tmpDir,"data2");
  FileUtils.writeLines(dataFile2,ImmutableList.of("2014102202,c.example.com,c.example.com,70"));
  String inputPath=tmpDir.getPath() + "/{data1,data2}";
  HadoopDruidIndexerConfig config=makeHadoopDruidIndexerConfig(ImmutableMap.<String,Object>of("type","multi","children",ImmutableList.of(ImmutableMap.<String,Object>of("type","dataSource","ingestionSpec",ImmutableMap.of("dataSource","xyz","interval",interval),"segments",segments),ImmutableMap.<String,Object>of("type","static","paths",inputPath))),temporaryFolder.newFolder());
  List<ImmutableMap<String,Object>> expectedRows=ImmutableList.of(ImmutableMap.<String,Object>of("time",DateTime.parse("2014-10-22T00:00:00.000Z"),"host",ImmutableList.of("a.example.com"),"visited_sum",190L,"unique_hosts",1.0d),ImmutableMap.<String,Object>of("time",DateTime.parse("2014-10-22T01:00:00.000Z"),"host",ImmutableList.of("b.example.com"),"visited_sum",175L,"unique_hosts",1.0d),ImmutableMap.<String,Object>of("time",DateTime.parse("2014-10-22T02:00:00.000Z"),"host",ImmutableList.of("c.example.com"),"visited_sum",270L,"unique_hosts",1.0d));
  testIngestion(config,expectedRows);
}
