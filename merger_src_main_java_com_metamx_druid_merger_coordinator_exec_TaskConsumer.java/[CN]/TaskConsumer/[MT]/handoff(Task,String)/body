{
  final TaskContext context=new TaskContext(version,ImmutableSet.copyOf(mergerDBCoordinator.getUsedSegmentsForInterval(task.getDataSource(),task.getInterval())),ImmutableSet.copyOf(mergerDBCoordinator.getUnusedSegmentsForInterval(task.getDataSource(),task.getInterval())));
  final ServiceMetricEvent.Builder builder=new ServiceMetricEvent.Builder().setUser2(task.getDataSource()).setUser4(task.getType().toString()).setUser5(task.getInterval().toString());
  TaskStatus preflightStatus;
  try {
    preflightStatus=task.preflight(context);
    log.info("Preflight done for task: %s",task.getId());
  }
 catch (  Exception e) {
    preflightStatus=TaskStatus.failure(task.getId());
    log.error(e,"Exception thrown during preflight for task: %s",task.getId());
  }
  if (!preflightStatus.isRunnable()) {
    log.info("Task finished during preflight: %s",task.getId());
    queue.notify(task,preflightStatus);
    return;
  }
  runner.run(task,context,new TaskCallback(){
    @Override public void notify(    final TaskStatus statusFromRunner){
      try {
        log.info("Received %s status for task: %s",statusFromRunner.getStatusCode(),task);
        if (shutdown) {
          log.info("Abandoning task due to shutdown: %s",task.getId());
          return;
        }
        queue.notify(task,statusFromRunner,new Runnable(){
          @Override public void run(){
            try {
              for (              final DataSegment segment : statusFromRunner.getSegments()) {
                verifyDataSourceAndInterval(task,context,segment);
                if (!context.getVersion().equals(segment.getVersion())) {
                  throw new IllegalStateException(String.format("Segment for task[%s] has invalid version: %s",task.getId(),segment.getIdentifier()));
                }
              }
              for (              final DataSegment segment : statusFromRunner.getSegmentsNuked()) {
                verifyDataSourceAndInterval(task,context,segment);
                if (segment.getVersion().compareTo(context.getVersion()) >= 0) {
                  throw new IllegalStateException(String.format("Segment-to-nuke for task[%s] has invalid version: %s",task.getId(),segment.getIdentifier()));
                }
              }
              mergerDBCoordinator.commitTaskStatus(statusFromRunner);
            }
 catch (            Exception e) {
              log.error(e,"Exception while publishing segments for task: %s",task);
              throw Throwables.propagate(e);
            }
          }
        }
);
        if (statusFromRunner.isComplete()) {
          builder.setUser3(statusFromRunner.getStatusCode().toString());
          for (          DataSegment segment : statusFromRunner.getSegments()) {
            emitter.emit(builder.build("indexer/segment/bytes",segment.getSize()));
          }
          for (          DataSegment segmentNuked : statusFromRunner.getSegmentsNuked()) {
            emitter.emit(builder.build("indexer/segmentNuked/bytes",segmentNuked.getSize()));
          }
          emitter.emit(builder.build("indexer/time/run/millis",statusFromRunner.getDuration()));
          if (statusFromRunner.isFailure()) {
            log.makeAlert("Failed to index").addData("task",task.getId()).addData("type",task.getType().toString()).addData("dataSource",task.getDataSource()).addData("interval",task.getInterval()).emit();
          }
          log.info("Task %s: %s (%d segments) (%d run duration)",statusFromRunner.getStatusCode(),task,statusFromRunner.getSegments().size(),statusFromRunner.getDuration());
        }
      }
 catch (      Exception e) {
        log.makeAlert(e,"Failed to handle task callback").addData("task",task.getId()).addData("statusCode",statusFromRunner.getStatusCode()).emit();
      }
    }
  }
);
}
