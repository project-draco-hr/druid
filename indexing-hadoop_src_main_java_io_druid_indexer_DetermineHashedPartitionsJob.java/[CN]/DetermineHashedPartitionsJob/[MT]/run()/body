{
  try {
    long startTime=System.currentTimeMillis();
    final Job groupByJob=new Job(new Configuration(),String.format("%s-determine_partitions_hashed-%s",config.getDataSource(),config.getIntervals()));
    JobHelper.injectSystemProperties(groupByJob);
    groupByJob.setInputFormatClass(TextInputFormat.class);
    groupByJob.setMapperClass(DetermineCardinalityMapper.class);
    groupByJob.setMapOutputKeyClass(LongWritable.class);
    groupByJob.setMapOutputValueClass(BytesWritable.class);
    groupByJob.setReducerClass(DetermineCardinalityReducer.class);
    groupByJob.setOutputKeyClass(NullWritable.class);
    groupByJob.setOutputValueClass(NullWritable.class);
    groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);
    groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);
    if (!config.getSegmentGranularIntervals().isPresent()) {
      groupByJob.setNumReduceTasks(1);
    }
 else {
      groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());
    }
    JobHelper.setupClasspath(config,groupByJob);
    config.addInputPaths(groupByJob);
    config.addJobProperties(groupByJob);
    config.intoConfiguration(groupByJob);
    FileOutputFormat.setOutputPath(groupByJob,config.makeGroupedDataDir());
    groupByJob.submit();
    log.info("Job %s submitted, status available at: %s",groupByJob.getJobName(),groupByJob.getTrackingURL());
    if (!groupByJob.waitForCompletion(true)) {
      log.error("Job failed: %s",groupByJob.getJobID());
      return false;
    }
    log.info("Job completed, loading up partitions for intervals[%s].",config.getSegmentGranularIntervals());
    FileSystem fileSystem=null;
    if (!config.getSegmentGranularIntervals().isPresent()) {
      final Path intervalInfoPath=config.makeIntervalInfoPath();
      fileSystem=intervalInfoPath.getFileSystem(groupByJob.getConfiguration());
      if (!Utils.exists(groupByJob,fileSystem,intervalInfoPath)) {
        throw new ISE("Path[%s] didn't exist!?",intervalInfoPath);
      }
      List<Interval> intervals=config.jsonMapper.readValue(Utils.openInputStream(groupByJob,intervalInfoPath),new TypeReference<List<Interval>>(){
      }
);
      config.setGranularitySpec(new UniformGranularitySpec(config.getGranularitySpec().getGranularity(),intervals));
      log.info("Determined Intervals for Job [%s]" + config.getSegmentGranularIntervals());
    }
    Map<DateTime,List<HadoopyShardSpec>> shardSpecs=Maps.newTreeMap(DateTimeComparator.getInstance());
    int shardCount=0;
    for (    Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {
      DateTime bucket=segmentGranularity.getStart();
      final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(segmentGranularity);
      if (fileSystem == null) {
        fileSystem=partitionInfoPath.getFileSystem(groupByJob.getConfiguration());
      }
      if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {
        Long cardinality=config.jsonMapper.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),new TypeReference<Long>(){
        }
);
        final int numberOfShards=(int)Math.ceil((double)cardinality / config.getTargetPartitionSize());
        List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);
        if (numberOfShards == 1) {
          actualSpecs.add(new HadoopyShardSpec(new NoneShardSpec(),shardCount++));
        }
 else {
          for (int i=0; i < numberOfShards; ++i) {
            actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards),shardCount++));
            log.info("DateTime[%s], partition[%d], spec[%s]",bucket,i,actualSpecs.get(i));
          }
        }
        shardSpecs.put(bucket,actualSpecs);
      }
 else {
        log.info("Path[%s] didn't exist!?",partitionInfoPath);
      }
    }
    config.setShardSpecs(shardSpecs);
    log.info("DetermineHashedPartitionsJob took %d millis",(System.currentTimeMillis() - startTime));
    return true;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}
