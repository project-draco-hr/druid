{
  SortableBytes keyBytes=SortableBytes.fromBytesWritable(key);
  Bucket bucket=Bucket.fromGroupKey(keyBytes.getGroupKey()).lhs;
  final Interval interval=config.getGranularitySpec().bucketInterval(bucket.time).get();
  final DataRollupSpec rollupSpec=config.getRollupSpec();
  final AggregatorFactory[] aggs=rollupSpec.getAggs().toArray(new AggregatorFactory[rollupSpec.getAggs().size()]);
  IncrementalIndex index=makeIncrementalIndex(bucket,aggs);
  File baseFlushFile=File.createTempFile("base","flush");
  baseFlushFile.delete();
  baseFlushFile.mkdirs();
  Set<File> toMerge=Sets.newTreeSet();
  int indexCount=0;
  int lineCount=0;
  int runningTotalLineCount=0;
  long startTime=System.currentTimeMillis();
  Set<String> allDimensionNames=Sets.newHashSet();
  for (  final Text value : values) {
    context.progress();
    final InputRow inputRow=parser.parse(value.toString());
    allDimensionNames.addAll(inputRow.getDimensions());
    int numRows=index.add(inputRow);
    ++lineCount;
    if (numRows >= rollupSpec.rowFlushBoundary) {
      log.info("%,d lines to %,d rows in %,d millis",lineCount - runningTotalLineCount,numRows,System.currentTimeMillis() - startTime);
      runningTotalLineCount=lineCount;
      final File file=new File(baseFlushFile,String.format("index%,05d",indexCount));
      toMerge.add(file);
      context.progress();
      IndexMerger.persist(index,interval,file,new IndexMerger.ProgressIndicator(){
        @Override public void progress(){
          context.progress();
        }
      }
);
      index=makeIncrementalIndex(bucket,aggs);
      startTime=System.currentTimeMillis();
      ++indexCount;
    }
  }
  log.info("%,d lines completed.",lineCount);
  List<QueryableIndex> indexes=Lists.newArrayListWithCapacity(indexCount);
  final File mergedBase;
  if (toMerge.size() == 0) {
    mergedBase=new File(baseFlushFile,"merged");
    IndexMerger.persist(index,interval,mergedBase,new IndexMerger.ProgressIndicator(){
      @Override public void progress(){
        context.progress();
      }
    }
);
  }
 else {
    final File finalFile=new File(baseFlushFile,"final");
    IndexMerger.persist(index,interval,finalFile,new IndexMerger.ProgressIndicator(){
      @Override public void progress(){
        context.progress();
      }
    }
);
    toMerge.add(finalFile);
    for (    File file : toMerge) {
      indexes.add(IndexIO.loadIndex(file));
    }
    mergedBase=IndexMerger.mergeQueryableIndex(indexes,aggs,new File(baseFlushFile,"merged"),new IndexMerger.ProgressIndicator(){
      @Override public void progress(){
        context.progress();
      }
    }
);
  }
  serializeOutIndex(context,bucket,mergedBase,Lists.newArrayList(allDimensionNames));
  for (  File file : toMerge) {
    FileUtils.deleteDirectory(file);
  }
}
