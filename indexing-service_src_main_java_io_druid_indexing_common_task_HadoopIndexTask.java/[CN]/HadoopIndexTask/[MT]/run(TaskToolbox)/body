{
  final ClassLoader loader=buildClassLoader(toolbox);
  boolean determineIntervals=!spec.getDataSchema().getGranularitySpec().bucketIntervals().isPresent();
  spec=HadoopIngestionSpec.updateSegmentListIfDatasourcePathSpecIsUsed(spec,jsonMapper,new OverlordActionBasedUsedSegmentLister(toolbox));
  final String config=invokeForeignLoader("io.druid.indexing.common.task.HadoopIndexTask$HadoopDetermineConfigInnerProcessing",new String[]{toolbox.getObjectMapper().writeValueAsString(spec),toolbox.getConfig().getHadoopWorkingPath(),toolbox.getSegmentPusher().getPathForHadoop()},loader);
  final HadoopIngestionSpec indexerSchema=toolbox.getObjectMapper().readValue(config,HadoopIngestionSpec.class);
  final String version;
  if (determineIntervals) {
    Interval interval=JodaUtils.umbrellaInterval(JodaUtils.condenseIntervals(indexerSchema.getDataSchema().getGranularitySpec().bucketIntervals().get()));
    TaskLock lock=toolbox.getTaskActionClient().submit(new LockAcquireAction(interval));
    version=lock.getVersion();
  }
 else {
    Iterable<TaskLock> locks=getTaskLocks(toolbox);
    final TaskLock myLock=Iterables.getOnlyElement(locks);
    version=myLock.getVersion();
  }
  log.info("Setting version to: %s",version);
  final String segments=invokeForeignLoader("io.druid.indexing.common.task.HadoopIndexTask$HadoopIndexGeneratorInnerProcessing",new String[]{toolbox.getObjectMapper().writeValueAsString(indexerSchema),version},loader);
  if (segments != null) {
    List<DataSegment> publishedSegments=toolbox.getObjectMapper().readValue(segments,new TypeReference<List<DataSegment>>(){
    }
);
    toolbox.publishSegments(publishedSegments);
    return TaskStatus.success(getId());
  }
 else {
    return TaskStatus.failure(getId());
  }
}
