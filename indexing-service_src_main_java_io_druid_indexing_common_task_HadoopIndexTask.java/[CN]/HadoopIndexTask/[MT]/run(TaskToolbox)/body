{
  final HadoopDruidIndexerConfig configCopy=toolbox.getObjectMapper().readValue(toolbox.getObjectMapper().writeValueAsBytes(config),HadoopDruidIndexerConfig.class);
  final TaskLock myLock=Iterables.getOnlyElement(getTaskLocks(toolbox));
  log.info("Setting version to: %s",myLock.getVersion());
  configCopy.setVersion(myLock.getVersion());
  configCopy.setJobOutputDir(toolbox.getConfig().getHadoopWorkingPath());
  if (toolbox.getSegmentPusher() instanceof S3DataSegmentPusher) {
    S3DataSegmentPusher segmentPusher=(S3DataSegmentPusher)toolbox.getSegmentPusher();
    String s3Path=String.format("s3n://%s/%s/%s",segmentPusher.getConfig().getBucket(),segmentPusher.getConfig().getBaseKey(),getDataSource());
    log.info("Setting segment output path to: %s",s3Path);
    configCopy.setSegmentOutputDir(s3Path);
  }
 else {
    throw new IllegalStateException("Sorry, we only work with S3DataSegmentPushers! Bummer!");
  }
  HadoopDruidIndexerJob job=new HadoopDruidIndexerJob(configCopy);
  configCopy.verify();
  log.info("Starting a hadoop index generator job...");
  if (job.run()) {
    List<DataSegment> publishedSegments=job.getPublishedSegments();
    toolbox.getTaskActionClient().submit(new SegmentInsertAction(ImmutableSet.copyOf(publishedSegments)));
    return TaskStatus.success(getId());
  }
 else {
    return TaskStatus.failure(getId());
  }
}
