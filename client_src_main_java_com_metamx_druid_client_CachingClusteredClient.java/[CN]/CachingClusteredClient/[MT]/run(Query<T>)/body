{
  final QueryToolChest<T,Query<T>> toolChest=warehouse.getToolChest(query);
  final CacheStrategy<T,Query<T>> strategy=toolChest.getCacheStrategy(query);
  final Map<DruidServer,List<SegmentDescriptor>> segs=Maps.newTreeMap();
  final List<Pair<DateTime,byte[]>> cachedResults=Lists.newArrayList();
  final Map<String,CachePopulator> cachePopulatorMap=Maps.newHashMap();
  final boolean useCache=Boolean.parseBoolean(query.getContextValue("useCache","true")) && strategy != null;
  final boolean populateCache=Boolean.parseBoolean(query.getContextValue("populateCache","true")) && strategy != null;
  final boolean isBySegment=Boolean.parseBoolean(query.getContextValue("bySegment","false"));
  final Query<T> rewrittenQuery;
  if (populateCache) {
    rewrittenQuery=query.withOverriddenContext(ImmutableMap.of("bySegment","true","intermediate","true"));
  }
 else {
    rewrittenQuery=query.withOverriddenContext(ImmutableMap.of("intermediate","true"));
  }
  VersionedIntervalTimeline<String,ServerSelector> timeline=serverView.getTimeline(query.getDataSource());
  if (timeline == null) {
    return Sequences.empty();
  }
  byte[] queryCacheKey=null;
  if (strategy != null) {
    queryCacheKey=strategy.computeCacheKey(query);
  }
  for (  Interval interval : rewrittenQuery.getIntervals()) {
    List<TimelineObjectHolder<String,ServerSelector>> serversLookup=timeline.lookup(interval);
    for (    TimelineObjectHolder<String,ServerSelector> holder : serversLookup) {
      for (      PartitionChunk<ServerSelector> chunk : holder.getObject()) {
        ServerSelector selector=chunk.getObject();
        final SegmentDescriptor descriptor=new SegmentDescriptor(holder.getInterval(),holder.getVersion(),chunk.getChunkNumber());
        if (queryCacheKey == null) {
          final DruidServer server=selector.pick();
          List<SegmentDescriptor> descriptors=segs.get(server);
          if (descriptors == null) {
            descriptors=Lists.newArrayList();
            segs.put(server,descriptors);
          }
          descriptors.add(descriptor);
        }
 else {
          final Interval segmentQueryInterval=holder.getInterval();
          final byte[] versionBytes=descriptor.getVersion().getBytes();
          final byte[] cacheKey=ByteBuffer.allocate(16 + versionBytes.length + 4+ queryCacheKey.length).putLong(segmentQueryInterval.getStartMillis()).putLong(segmentQueryInterval.getEndMillis()).put(versionBytes).putInt(descriptor.getPartitionNumber()).put(queryCacheKey).array();
          final String segmentIdentifier=selector.getSegment().getIdentifier();
          final Cache cache=cacheBroker.provideCache(segmentIdentifier);
          final byte[] cachedValue=cache.get(cacheKey);
          if (useCache && cachedValue != null) {
            cachedResults.add(Pair.of(segmentQueryInterval.getStart(),cachedValue));
          }
 else {
            final DruidServer server=selector.pick();
            List<SegmentDescriptor> descriptors=segs.get(server);
            if (descriptors == null) {
              descriptors=Lists.newArrayList();
              segs.put(server,descriptors);
            }
            descriptors.add(descriptor);
            cachePopulatorMap.put(String.format("%s_%s",segmentIdentifier,segmentQueryInterval),new CachePopulator(cache,objectMapper,cacheKey));
          }
        }
      }
    }
  }
  return new LazySequence<T>(new Supplier<Sequence<T>>(){
    @Override public Sequence<T> get(){
      ArrayList<Pair<DateTime,Sequence<T>>> listOfSequences=Lists.newArrayList();
      addSequencesFromServer(listOfSequences);
      addSequencesFromCache(listOfSequences);
      Collections.sort(listOfSequences,Ordering.natural().onResultOf(Pair.<DateTime,Sequence<T>>lhsFn()));
      final Sequence<Sequence<T>> seq=Sequences.simple(Iterables.transform(listOfSequences,Pair.<DateTime,Sequence<T>>rhsFn()));
      if (strategy == null) {
        return toolChest.mergeSequences(seq);
      }
 else {
        return strategy.mergeSequences(seq);
      }
    }
    private void addSequencesFromCache(    ArrayList<Pair<DateTime,Sequence<T>>> listOfSequences){
      if (strategy == null) {
        return;
      }
      final Function<Object,T> pullFromCacheFunction=strategy.pullFromCache();
      for (      Pair<DateTime,byte[]> cachedResultPair : cachedResults) {
        final byte[] cachedResult=cachedResultPair.rhs;
        Sequence<Object> cachedSequence=new BaseSequence<Object,Iterator<Object>>(new BaseSequence.IteratorMaker<Object,Iterator<Object>>(){
          @Override public Iterator<Object> make(){
            try {
              return objectMapper.readValues(objectMapper.getJsonFactory().createJsonParser(cachedResult),Object.class);
            }
 catch (            IOException e) {
              throw Throwables.propagate(e);
            }
          }
          @Override public void cleanup(          Iterator<Object> iterFromMake){
          }
        }
);
        listOfSequences.add(Pair.of(cachedResultPair.lhs,Sequences.map(cachedSequence,pullFromCacheFunction)));
      }
    }
    @SuppressWarnings("unchecked") private void addSequencesFromServer(    ArrayList<Pair<DateTime,Sequence<T>>> listOfSequences){
      for (      Map.Entry<DruidServer,List<SegmentDescriptor>> entry : segs.entrySet()) {
        final DruidServer server=entry.getKey();
        final List<SegmentDescriptor> descriptors=entry.getValue();
        final QueryRunner clientQueryable=serverView.getQueryRunner(server);
        if (clientQueryable == null) {
          throw new ISE("WTF!? server[%s] doesn't have a client Queryable?",server);
        }
        final Sequence<T> resultSeqToAdd;
        final MultipleSpecificSegmentSpec segmentSpec=new MultipleSpecificSegmentSpec(descriptors);
        List<Interval> intervals=segmentSpec.getIntervals();
        if ("realtime".equals(server.getType()) || !populateCache || isBySegment) {
          resultSeqToAdd=clientQueryable.run(query.withQuerySegmentSpec(segmentSpec));
        }
 else {
          resultSeqToAdd=toolChest.mergeSequences(Sequences.map(clientQueryable.run(rewrittenQuery.withQuerySegmentSpec(segmentSpec)),new Function<Object,Sequence<T>>(){
            private final Function<T,Object> prepareForCache=strategy.prepareForCache();
            @Override public Sequence<T> apply(            Object input){
              Result<Object> result=(Result<Object>)input;
              final BySegmentResultValueClass<T> value=(BySegmentResultValueClass<T>)result.getValue();
              String segmentIdentifier=value.getSegmentId();
              final Iterable<T> segmentResults=value.getResults();
              cachePopulatorMap.get(String.format("%s_%s",segmentIdentifier,value.getIntervalString())).populate(Iterables.transform(segmentResults,prepareForCache));
              return Sequences.simple(Iterables.transform(segmentResults,toolChest.makeMetricManipulatorFn(rewrittenQuery,new MetricManipulationFn(){
                @Override public Object manipulate(                AggregatorFactory factory,                Object object){
                  return factory.deserialize(object);
                }
              }
)));
            }
          }
));
        }
        listOfSequences.add(Pair.of(intervals.get(0).getStart(),resultSeqToAdd));
      }
    }
  }
);
}
