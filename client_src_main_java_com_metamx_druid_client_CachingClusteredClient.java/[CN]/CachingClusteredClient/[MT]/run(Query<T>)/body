{
  final QueryToolChest<T,Query<T>> toolChest=warehouse.getToolChest(query);
  final CacheStrategy<T,Query<T>> strategy=toolChest.getCacheStrategy(query);
  final Map<DruidServer,List<SegmentDescriptor>> serverSegments=Maps.newTreeMap();
  final List<Pair<DateTime,byte[]>> cachedResults=Lists.newArrayList();
  final Map<String,CachePopulator> cachePopulatorMap=Maps.newHashMap();
  final boolean useCache=Boolean.parseBoolean(query.getContextValue("useCache","true")) && strategy != null;
  final boolean populateCache=Boolean.parseBoolean(query.getContextValue("populateCache","true")) && strategy != null;
  final boolean isBySegment=Boolean.parseBoolean(query.getContextValue("bySegment","false"));
  final Query<T> rewrittenQuery;
  if (populateCache) {
    rewrittenQuery=query.withOverriddenContext(ImmutableMap.of("bySegment","true","intermediate","true"));
  }
 else {
    rewrittenQuery=query.withOverriddenContext(ImmutableMap.of("intermediate","true"));
  }
  VersionedIntervalTimeline<String,ServerSelector> timeline=serverView.getTimeline(query.getDataSource());
  if (timeline == null) {
    return Sequences.empty();
  }
  Set<Pair<ServerSelector,SegmentDescriptor>> segments=Sets.newLinkedHashSet();
  for (  Interval interval : rewrittenQuery.getIntervals()) {
    List<TimelineObjectHolder<String,ServerSelector>> serversLookup=timeline.lookup(interval);
    for (    TimelineObjectHolder<String,ServerSelector> holder : serversLookup) {
      for (      PartitionChunk<ServerSelector> chunk : holder.getObject()) {
        ServerSelector selector=chunk.getObject();
        final SegmentDescriptor descriptor=new SegmentDescriptor(holder.getInterval(),holder.getVersion(),chunk.getChunkNumber());
        segments.add(Pair.of(selector,descriptor));
      }
    }
  }
  final byte[] queryCacheKey;
  if (strategy != null) {
    queryCacheKey=strategy.computeCacheKey(query);
  }
 else {
    queryCacheKey=null;
  }
  if (useCache && queryCacheKey != null) {
    Map<Pair<ServerSelector,SegmentDescriptor>,CacheBroker.NamedKey> cacheKeys=Maps.newHashMap();
    for (    Pair<ServerSelector,SegmentDescriptor> e : segments) {
      cacheKeys.put(e,computeSegmentCacheKey(e.lhs.getSegment().getIdentifier(),e.rhs,queryCacheKey));
    }
    Map<CacheBroker.NamedKey,byte[]> cachedValues=cacheBroker.getBulk(cacheKeys.values());
    for (    Map.Entry<Pair<ServerSelector,SegmentDescriptor>,CacheBroker.NamedKey> entry : cacheKeys.entrySet()) {
      Pair<ServerSelector,SegmentDescriptor> segment=entry.getKey();
      CacheBroker.NamedKey segmentCacheKey=entry.getValue();
      final ServerSelector selector=segment.lhs;
      final SegmentDescriptor descriptor=segment.rhs;
      final Interval segmentQueryInterval=descriptor.getInterval();
      final byte[] cachedValue=cachedValues.get(segmentCacheKey);
      if (cachedValue != null) {
        cachedResults.add(Pair.of(segmentQueryInterval.getStart(),cachedValue));
        segments.remove(segment);
      }
 else {
        final String segmentIdentifier=selector.getSegment().getIdentifier();
        cachePopulatorMap.put(String.format("%s_%s",segmentIdentifier,segmentQueryInterval),new CachePopulator(cacheBroker,objectMapper,segmentCacheKey));
      }
    }
  }
  for (  Pair<ServerSelector,SegmentDescriptor> segment : segments) {
    final DruidServer server=segment.lhs.pick();
    List<SegmentDescriptor> descriptors=serverSegments.get(server);
    if (descriptors == null) {
      descriptors=Lists.newArrayList();
      serverSegments.put(server,descriptors);
    }
    descriptors.add(segment.rhs);
  }
  return new LazySequence<T>(new Supplier<Sequence<T>>(){
    @Override public Sequence<T> get(){
      ArrayList<Pair<DateTime,Sequence<T>>> listOfSequences=Lists.newArrayList();
      addSequencesFromServer(listOfSequences);
      addSequencesFromCache(listOfSequences);
      Collections.sort(listOfSequences,Ordering.natural().onResultOf(Pair.<DateTime,Sequence<T>>lhsFn()));
      final Sequence<Sequence<T>> seq=Sequences.simple(Iterables.transform(listOfSequences,Pair.<DateTime,Sequence<T>>rhsFn()));
      if (strategy == null) {
        return toolChest.mergeSequences(seq);
      }
 else {
        return strategy.mergeSequences(seq);
      }
    }
    private void addSequencesFromCache(    ArrayList<Pair<DateTime,Sequence<T>>> listOfSequences){
      if (strategy == null) {
        return;
      }
      final Function<Object,T> pullFromCacheFunction=strategy.pullFromCache();
      for (      Pair<DateTime,byte[]> cachedResultPair : cachedResults) {
        final byte[] cachedResult=cachedResultPair.rhs;
        Sequence<Object> cachedSequence=new BaseSequence<Object,Iterator<Object>>(new BaseSequence.IteratorMaker<Object,Iterator<Object>>(){
          @Override public Iterator<Object> make(){
            try {
              if (cachedResult.length == 0) {
                return Iterators.emptyIterator();
              }
              return objectMapper.readValues(objectMapper.getJsonFactory().createJsonParser(cachedResult),Object.class);
            }
 catch (            IOException e) {
              throw Throwables.propagate(e);
            }
          }
          @Override public void cleanup(          Iterator<Object> iterFromMake){
          }
        }
);
        listOfSequences.add(Pair.of(cachedResultPair.lhs,Sequences.map(cachedSequence,pullFromCacheFunction)));
      }
    }
    @SuppressWarnings("unchecked") private void addSequencesFromServer(    ArrayList<Pair<DateTime,Sequence<T>>> listOfSequences){
      for (      Map.Entry<DruidServer,List<SegmentDescriptor>> entry : serverSegments.entrySet()) {
        final DruidServer server=entry.getKey();
        final List<SegmentDescriptor> descriptors=entry.getValue();
        final QueryRunner clientQueryable=serverView.getQueryRunner(server);
        if (clientQueryable == null) {
          throw new ISE("WTF!? server[%s] doesn't have a client Queryable?",server);
        }
        final Sequence<T> resultSeqToAdd;
        final MultipleSpecificSegmentSpec segmentSpec=new MultipleSpecificSegmentSpec(descriptors);
        List<Interval> intervals=segmentSpec.getIntervals();
        if ("realtime".equals(server.getType()) || !populateCache || isBySegment) {
          resultSeqToAdd=clientQueryable.run(query.withQuerySegmentSpec(segmentSpec));
        }
 else {
          resultSeqToAdd=toolChest.mergeSequences(Sequences.map(clientQueryable.run(rewrittenQuery.withQuerySegmentSpec(segmentSpec)),new Function<Object,Sequence<T>>(){
            private final Function<T,Object> prepareForCache=strategy.prepareForCache();
            @Override public Sequence<T> apply(            Object input){
              Result<Object> result=(Result<Object>)input;
              final BySegmentResultValueClass<T> value=(BySegmentResultValueClass<T>)result.getValue();
              String segmentIdentifier=value.getSegmentId();
              final Iterable<T> segmentResults=value.getResults();
              cachePopulatorMap.get(String.format("%s_%s",segmentIdentifier,value.getInterval())).populate(Iterables.transform(segmentResults,prepareForCache));
              return Sequences.simple(Iterables.transform(segmentResults,toolChest.makeMetricManipulatorFn(rewrittenQuery,new MetricManipulationFn(){
                @Override public Object manipulate(                AggregatorFactory factory,                Object object){
                  return factory.deserialize(object);
                }
              }
)));
            }
          }
));
        }
        listOfSequences.add(Pair.of(intervals.get(0).getStart(),resultSeqToAdd));
      }
    }
  }
);
}
