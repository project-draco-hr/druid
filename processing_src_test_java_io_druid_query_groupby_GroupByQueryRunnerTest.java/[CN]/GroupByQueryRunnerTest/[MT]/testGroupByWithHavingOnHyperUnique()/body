{
  GroupByQuery query=new GroupByQuery.Builder().setDataSource(QueryRunnerTestHelper.dataSource).setGranularity(QueryRunnerTestHelper.allGran).setDimensions(Arrays.<DimensionSpec>asList(new DefaultDimensionSpec(QueryRunnerTestHelper.marketDimension,QueryRunnerTestHelper.marketDimension))).setInterval(QueryRunnerTestHelper.fullOnInterval).setLimitSpec(new DefaultLimitSpec(Lists.newArrayList(new OrderByColumnSpec(QueryRunnerTestHelper.uniqueMetric,OrderByColumnSpec.Direction.DESCENDING)),3)).setHavingSpec(new GreaterThanHavingSpec(QueryRunnerTestHelper.uniqueMetric,8)).setAggregatorSpecs(Lists.<AggregatorFactory>newArrayList(QueryRunnerTestHelper.qualityUniques)).setPostAggregatorSpecs(Lists.<PostAggregator>newArrayList(new HyperUniqueFinalizingPostAggregator(QueryRunnerTestHelper.hyperUniqueFinalizingPostAggMetric,QueryRunnerTestHelper.uniqueMetric))).build();
  List<Row> expectedResults=Arrays.asList(GroupByQueryRunnerTestHelper.createExpectedRow("1970-01-01T00:00:00.000Z","market","spot",QueryRunnerTestHelper.uniqueMetric,QueryRunnerTestHelper.UNIQUES_9,QueryRunnerTestHelper.hyperUniqueFinalizingPostAggMetric,QueryRunnerTestHelper.UNIQUES_9));
  expectedException.expect(ParseException.class);
  expectedException.expectMessage("Unknown type[class io.druid.query.aggregation.hyperloglog.HLLCV1]");
  Iterable<Row> results=GroupByQueryRunnerTestHelper.runQuery(factory,runner,query);
  TestHelper.assertExpectedObjects(expectedResults,results,"order-limit");
}
