{
  try {
    if (!config.getPartitionsSpec().isAssumeGrouped()) {
      final Job groupByJob=new Job(new Configuration(),String.format("%s-determine_partitions_groupby-%s",config.getDataSource(),config.getIntervals()));
      injectSystemProperties(groupByJob);
      groupByJob.setInputFormatClass(TextInputFormat.class);
      groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);
      groupByJob.setMapOutputKeyClass(BytesWritable.class);
      groupByJob.setMapOutputValueClass(NullWritable.class);
      groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);
      groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);
      groupByJob.setOutputKeyClass(BytesWritable.class);
      groupByJob.setOutputValueClass(NullWritable.class);
      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);
      groupByJob.setJarByClass(DeterminePartitionsJob.class);
      config.addInputPaths(groupByJob);
      config.intoConfiguration(groupByJob);
      FileOutputFormat.setOutputPath(groupByJob,config.makeGroupedDataDir());
      groupByJob.submit();
      log.info("Job %s submitted, status available at: %s",groupByJob.getJobName(),groupByJob.getTrackingURL());
      if (!groupByJob.waitForCompletion(true)) {
        log.error("Job failed: %s",groupByJob.getJobID().toString());
        return false;
      }
    }
 else {
      log.info("Skipping group-by job.");
    }
    final Job dimSelectionJob=new Job(new Configuration(),String.format("%s-determine_partitions_dimselection-%s",config.getDataSource(),config.getIntervals()));
    dimSelectionJob.getConfiguration().set("io.sort.record.percent","0.19");
    injectSystemProperties(dimSelectionJob);
    if (!config.getPartitionsSpec().isAssumeGrouped()) {
      dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);
      dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);
      FileInputFormat.addInputPath(dimSelectionJob,config.makeGroupedDataDir());
    }
 else {
      dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);
      dimSelectionJob.setInputFormatClass(TextInputFormat.class);
      config.addInputPaths(dimSelectionJob);
    }
    SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob);
    dimSelectionJob.setMapOutputValueClass(Text.class);
    dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);
    dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);
    dimSelectionJob.setOutputKeyClass(BytesWritable.class);
    dimSelectionJob.setOutputValueClass(Text.class);
    dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);
    dimSelectionJob.setJarByClass(DeterminePartitionsJob.class);
    config.intoConfiguration(dimSelectionJob);
    FileOutputFormat.setOutputPath(dimSelectionJob,config.makeIntermediatePath());
    dimSelectionJob.submit();
    log.info("Job %s submitted, status available at: %s",dimSelectionJob.getJobName(),dimSelectionJob.getTrackingURL());
    if (!dimSelectionJob.waitForCompletion(true)) {
      log.error("Job failed: %s",dimSelectionJob.getJobID().toString());
      return false;
    }
    log.info("Job completed, loading up partitions for intervals[%s].",config.getSegmentGranularIntervals());
    FileSystem fileSystem=null;
    Map<DateTime,List<HadoopyShardSpec>> shardSpecs=Maps.newTreeMap(DateTimeComparator.getInstance());
    int shardCount=0;
    for (    Interval segmentGranularity : config.getSegmentGranularIntervals()) {
      DateTime bucket=segmentGranularity.getStart();
      final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(new Bucket(0,bucket,0));
      if (fileSystem == null) {
        fileSystem=partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());
      }
      if (fileSystem.exists(partitionInfoPath)) {
        List<ShardSpec> specs=config.jsonMapper.readValue(Utils.openInputStream(dimSelectionJob,partitionInfoPath),new TypeReference<List<ShardSpec>>(){
        }
);
        List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(specs.size());
        for (int i=0; i < specs.size(); ++i) {
          actualSpecs.add(new HadoopyShardSpec(specs.get(i),shardCount++));
          log.info("DateTime[%s], partition[%d], spec[%s]",bucket,i,actualSpecs.get(i));
        }
        shardSpecs.put(bucket,actualSpecs);
      }
 else {
        log.info("Path[%s] didn't exist!?",partitionInfoPath);
      }
    }
    config.setShardSpecs(shardSpecs);
    return true;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}
