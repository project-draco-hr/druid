{
  HadoopDruidIndexerConfig hadoopIndexerConfig=new HadoopDruidIndexerConfig(new HadoopIngestionSpec(new DataSchema(ingestionSpec.getDataSource(),new StringInputRowParser(new CSVParseSpec(new TimestampSpec("timestamp","yyyyMMddHH",null),new DimensionsSpec(null,null,null),null,ImmutableList.of("timestamp","host","visited"))),new AggregatorFactory[]{new LongSumAggregatorFactory("visited_sum","visited")},new UniformGranularitySpec(Granularity.DAY,QueryGranularity.NONE,ImmutableList.of(Interval.parse("2000/3000")))),new HadoopIOConfig(ImmutableMap.<String,Object>of("paths","/tmp/dummy","type","static"),null,"/tmp/dummy"),HadoopTuningConfig.makeDefaultTuningConfig().withWorkingPath("/tmp/work").withVersion("ver")));
  ObjectMapper mapper=new DefaultObjectMapper();
  DatasourcePathSpec pathSpec=new DatasourcePathSpec(mapper,segments,ingestionSpec,null);
  Configuration config=new Configuration();
  Job job=EasyMock.createNiceMock(Job.class);
  EasyMock.expect(job.getConfiguration()).andReturn(config).anyTimes();
  EasyMock.replay(job);
  pathSpec.addInputPaths(hadoopIndexerConfig,job);
  List<DataSegment> actualSegments=mapper.readValue(config.get(DatasourceInputFormat.CONF_INPUT_SEGMENTS),new TypeReference<List<DataSegment>>(){
  }
);
  Assert.assertEquals(segments,actualSegments);
  DatasourceIngestionSpec actualIngestionSpec=mapper.readValue(config.get(DatasourceInputFormat.CONF_DRUID_SCHEMA),DatasourceIngestionSpec.class);
  Assert.assertEquals(ingestionSpec.withDimensions(ImmutableList.of("product")).withMetrics(ImmutableList.of("visited_sum")),actualIngestionSpec);
}
