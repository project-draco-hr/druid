{
  Set<String> newDimExclus=Sets.union(firehoseParser.getParseSpec().getDimensionsSpec().getDimensionExclusions(),Sets.newHashSet("feed"));
  final ByteBufferInputRowParser theParser=firehoseParser.withParseSpec(firehoseParser.getParseSpec().withDimensionsSpec(firehoseParser.getParseSpec().getDimensionsSpec().withDimensionExclusions(newDimExclus)));
  final ConsumerConnector connector=Consumer.createJavaConsumerConnector(new ConsumerConfig(consumerProps));
  final Map<String,List<KafkaStream<byte[],byte[]>>> streams=connector.createMessageStreams(ImmutableMap.of(feed,1));
  final List<KafkaStream<byte[],byte[]>> streamList=streams.get(feed);
  if (streamList == null || streamList.size() != 1) {
    return null;
  }
  final KafkaStream<byte[],byte[]> stream=streamList.get(0);
  final ConsumerIterator<byte[],byte[]> iter=stream.iterator();
  return new Firehose(){
    @Override public boolean hasMore(){
      return iter.hasNext();
    }
    @Override public InputRow nextRow(){
      try {
        final byte[] message=iter.next().message();
        if (message == null) {
          return null;
        }
        return theParser.parse(ByteBuffer.wrap(message));
      }
 catch (      InvalidMessageException e) {
        log.error(e,"Message failed its checksum and it is corrupt, will skip it");
        return null;
      }
    }
    @Override public Runnable commit(){
      return new Runnable(){
        @Override public void run(){
          log.info("committing offsets");
          connector.commitOffsets();
        }
      }
;
    }
    @Override public void close() throws IOException {
      connector.shutdown();
    }
  }
;
}
