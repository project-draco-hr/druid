{
  log.info("Starting up!");
  startTime=DateTime.now();
  mapper=toolbox.getObjectMapper();
  status=Status.STARTING;
  if (chatHandlerProvider.isPresent()) {
    log.info("Found chat handler of class[%s]",chatHandlerProvider.get().getClass().getName());
    chatHandlerProvider.get().register(getId(),this);
  }
 else {
    log.warn("No chat handler detected");
  }
  runThread=Thread.currentThread();
  final FireDepartment fireDepartmentForMetrics=new FireDepartment(dataSchema,new RealtimeIOConfig(null,null,null),null);
  fireDepartmentMetrics=fireDepartmentForMetrics.getMetrics();
  toolbox.getMonitorScheduler().addMonitor(new RealtimeMetricsMonitor(ImmutableList.of(fireDepartmentForMetrics),ImmutableMap.of(DruidMetrics.TASK_ID,new String[]{getId()})));
  try (final Appenderator appenderator0=newAppenderator(fireDepartmentMetrics,toolbox);final FiniteAppenderatorDriver driver=newDriver(appenderator0,toolbox);final KafkaConsumer<byte[],byte[]> consumer=newConsumer()){
    appenderator=appenderator0;
    final String topic=ioConfig.getStartPartitions().getTopic();
    final Object restoredMetadata=driver.startJob();
    if (restoredMetadata == null) {
      nextOffsets.putAll(ioConfig.getStartPartitions().getPartitionOffsetMap());
    }
 else {
      final Map<String,Object> restoredMetadataMap=(Map)restoredMetadata;
      final KafkaPartitions restoredNextPartitions=toolbox.getObjectMapper().convertValue(restoredMetadataMap.get(METADATA_NEXT_PARTITIONS),KafkaPartitions.class);
      nextOffsets.putAll(restoredNextPartitions.getPartitionOffsetMap());
      if (!restoredNextPartitions.getTopic().equals(ioConfig.getStartPartitions().getTopic())) {
        throw new ISE("WTF?! Restored topic[%s] but expected topic[%s]",restoredNextPartitions.getTopic(),ioConfig.getStartPartitions().getTopic());
      }
      if (!nextOffsets.keySet().equals(ioConfig.getStartPartitions().getPartitionOffsetMap().keySet())) {
        throw new ISE("WTF?! Restored partitions[%s] but expected partitions[%s]",nextOffsets.keySet(),ioConfig.getStartPartitions().getPartitionOffsetMap().keySet());
      }
    }
    final Map<Integer,String> sequenceNames=Maps.newHashMap();
    for (    Integer partitionNum : nextOffsets.keySet()) {
      sequenceNames.put(partitionNum,String.format("%s_%s",ioConfig.getBaseSequenceName(),partitionNum));
    }
    final Supplier<Committer> committerSupplier=new Supplier<Committer>(){
      @Override public Committer get(){
        final Map<Integer,Long> snapshot=ImmutableMap.copyOf(nextOffsets);
        return new Committer(){
          @Override public Object getMetadata(){
            return ImmutableMap.of(METADATA_NEXT_PARTITIONS,new KafkaPartitions(ioConfig.getStartPartitions().getTopic(),snapshot));
          }
          @Override public void run(){
          }
        }
;
      }
    }
;
    Set<Integer> assignment=assignPartitionsAndSeekToNext(consumer,topic);
    boolean stillReading=!assignment.isEmpty();
    try {
      while (stillReading) {
        if (possiblyPause(assignment)) {
          assignment=assignPartitionsAndSeekToNext(consumer,topic);
          if (assignment.isEmpty()) {
            log.info("All partitions have been fully read");
            publishOnStop=true;
            stopRequested=true;
          }
        }
        if (stopRequested) {
          break;
        }
        final ConsumerRecords<byte[],byte[]> records=RetryUtils.retry(new Callable<ConsumerRecords<byte[],byte[]>>(){
          @Override public ConsumerRecords<byte[],byte[]> call() throws Exception {
            try {
              return consumer.poll(POLL_TIMEOUT);
            }
  finally {
              status=Status.READING;
            }
          }
        }
,new Predicate<Throwable>(){
          @Override public boolean apply(          Throwable input){
            return input instanceof OffsetOutOfRangeException;
          }
        }
,Integer.MAX_VALUE);
        for (        ConsumerRecord<byte[],byte[]> record : records) {
          if (log.isTraceEnabled()) {
            log.trace("Got topic[%s] partition[%d] offset[%,d].",record.topic(),record.partition(),record.offset());
          }
          if (record.offset() < endOffsets.get(record.partition())) {
            if (record.offset() != nextOffsets.get(record.partition())) {
              throw new ISE("WTF?! Got offset[%,d] after offset[%,d] in partition[%d].",record.offset(),nextOffsets.get(record.partition()),record.partition());
            }
            try {
              final InputRow row=Preconditions.checkNotNull(parser.parse(ByteBuffer.wrap(record.value())),"row");
              if (!ioConfig.getMinimumMessageTime().isPresent() || !ioConfig.getMinimumMessageTime().get().isAfter(row.getTimestamp())) {
                final SegmentIdentifier identifier=driver.add(row,sequenceNames.get(record.partition()),committerSupplier);
                if (identifier == null) {
                  throw new ISE("Could not allocate segment for row with timestamp[%s]",row.getTimestamp());
                }
                fireDepartmentMetrics.incrementProcessed();
              }
 else {
                fireDepartmentMetrics.incrementThrownAway();
              }
            }
 catch (            ParseException e) {
              if (tuningConfig.isReportParseExceptions()) {
                throw e;
              }
 else {
                log.debug(e,"Dropping unparseable row from partition[%d] offset[%,d].",record.partition(),record.offset());
                fireDepartmentMetrics.incrementUnparseable();
              }
            }
            nextOffsets.put(record.partition(),record.offset() + 1);
          }
          if (nextOffsets.get(record.partition()).equals(endOffsets.get(record.partition())) && assignment.remove(record.partition())) {
            log.info("Finished reading topic[%s], partition[%,d].",record.topic(),record.partition());
            assignPartitions(consumer,topic,assignment);
            stillReading=ioConfig.isPauseAfterRead() || !assignment.isEmpty();
          }
        }
      }
    }
  finally {
      driver.persist(committerSupplier.get());
    }
    if (stopRequested && !publishOnStop) {
      throw new InterruptedException("Stopping without publishing");
    }
    status=Status.PUBLISHING;
    final TransactionalSegmentPublisher publisher=new TransactionalSegmentPublisher(){
      @Override public boolean publishSegments(      Set<DataSegment> segments,      Object commitMetadata) throws IOException {
        final KafkaPartitions finalPartitions=toolbox.getObjectMapper().convertValue(((Map)commitMetadata).get(METADATA_NEXT_PARTITIONS),KafkaPartitions.class);
        if (!endOffsets.equals(finalPartitions.getPartitionOffsetMap())) {
          throw new ISE("WTF?! Driver attempted to publish invalid metadata[%s].",commitMetadata);
        }
        final SegmentTransactionalInsertAction action;
        if (ioConfig.isUseTransaction()) {
          action=new SegmentTransactionalInsertAction(segments,new KafkaDataSourceMetadata(ioConfig.getStartPartitions()),new KafkaDataSourceMetadata(finalPartitions));
        }
 else {
          action=new SegmentTransactionalInsertAction(segments,null,null);
        }
        log.info("Publishing with isTransaction[%s].",ioConfig.isUseTransaction());
        return toolbox.getTaskActionClient().submit(action).isSuccess();
      }
    }
;
    final SegmentsAndMetadata published=driver.finish(publisher,committerSupplier.get());
    if (published == null) {
      throw new ISE("Transaction failure publishing segments, aborting");
    }
 else {
      log.info("Published segments[%s] with metadata[%s].",Joiner.on(", ").join(Iterables.transform(published.getSegments(),new Function<DataSegment,String>(){
        @Override public String apply(        DataSegment input){
          return input.getIdentifier();
        }
      }
)),published.getCommitMetadata());
    }
  }
 catch (  InterruptedException e) {
    if (!stopRequested) {
      Thread.currentThread().interrupt();
      throw e;
    }
    log.info("The task was asked to stop before completing");
  }
  return success();
}
