{
  String dataSource="dataSource1";
  String tier="hot";
  Rule foreverLoadRule=new ForeverLoadRule(ImmutableMap.of(tier,2));
  EasyMock.expect(metadataRuleManager.getRulesWithDefault(EasyMock.anyString())).andReturn(ImmutableList.of(foreverLoadRule)).atLeastOnce();
  metadataRuleManager.stop();
  EasyMock.expectLastCall().once();
  EasyMock.replay(metadataRuleManager);
  DruidDataSource[] druidDataSources={new DruidDataSource(dataSource,new HashMap())};
  final DataSegment dataSegment=new DataSegment(dataSource,new Interval("2010-01-01/P1D"),"v1",null,null,null,null,0x9,0);
  druidDataSources[0].addSegment("0",dataSegment);
  EasyMock.expect(databaseSegmentManager.isStarted()).andReturn(true).anyTimes();
  EasyMock.expect(databaseSegmentManager.getInventory()).andReturn(ImmutableList.of(druidDataSources[0])).atLeastOnce();
  EasyMock.replay(databaseSegmentManager);
  ImmutableDruidDataSource immutableDruidDataSource=EasyMock.createNiceMock(ImmutableDruidDataSource.class);
  EasyMock.expect(immutableDruidDataSource.getSegments()).andReturn(ImmutableSet.of(dataSegment)).atLeastOnce();
  EasyMock.replay(immutableDruidDataSource);
  druidServer=new DruidServer("server1","localhost",5L,"historical",tier,0);
  loadManagementPeons.put("server1",loadQueuePeon);
  EasyMock.expect(serverInventoryView.getInventory()).andReturn(ImmutableList.of(druidServer)).atLeastOnce();
  try {
    serverInventoryView.start();
    EasyMock.expectLastCall().atLeastOnce();
  }
 catch (  Exception ex) {
    throw ex;
  }
  EasyMock.expect(serverInventoryView.isStarted()).andReturn(true).anyTimes();
  serverInventoryView.stop();
  EasyMock.expectLastCall().once();
  EasyMock.replay(serverInventoryView);
  coordinator.start();
  try {
    leaderAnnouncerLatch.await();
  }
 catch (  InterruptedException ex) {
    throw ex;
  }
  Assert.assertTrue(coordinator.isLeader());
  Assert.assertEquals(druidNode.getHostAndPort(),coordinator.getCurrentLeader());
  final CountDownLatch assignSegmentLatch=new CountDownLatch(1);
  pathChildrenCache.getListenable().addListener(new PathChildrenCacheListener(){
    @Override public void childEvent(    CuratorFramework curatorFramework,    PathChildrenCacheEvent pathChildrenCacheEvent) throws Exception {
      if (pathChildrenCacheEvent.getType().equals(PathChildrenCacheEvent.Type.CHILD_ADDED)) {
        druidServer.addDataSegment(dataSegment.getIdentifier(),dataSegment);
        assignSegmentLatch.countDown();
      }
    }
  }
);
  try {
    pathChildrenCache.start();
  }
 catch (  Exception ex) {
    throw ex;
  }
  Assert.assertTrue(assignSegmentLatch.await(2,TimeUnit.SECONDS));
  Assert.assertEquals(ImmutableMap.of(dataSource,100.0),coordinator.getLoadStatus());
  curator.delete().guaranteed().forPath(ZKPaths.makePath(LOADPATH,dataSegment.getIdentifier()));
  while (coordinator.getSegmentAvailability().snapshot().get(dataSource) != 0) {
    Thread.sleep(50);
  }
  Map segmentAvailability=coordinator.getSegmentAvailability().snapshot();
  Assert.assertEquals(1,segmentAvailability.size());
  Assert.assertEquals(0L,segmentAvailability.get(dataSource));
  while (coordinator.getLoadPendingDatasources().get(dataSource).get() > 0) {
    Thread.sleep(50);
  }
  Thread.sleep(druidCoordinatorConfig.getCoordinatorPeriod().getMillis());
  Map<String,CountingMap<String>> replicationStatus=coordinator.getReplicationStatus();
  Assert.assertNotNull(replicationStatus);
  Assert.assertEquals(1,replicationStatus.entrySet().size());
  CountingMap<String> dataSourceMap=replicationStatus.get(tier);
  Assert.assertNotNull(dataSourceMap);
  Assert.assertEquals(1,dataSourceMap.size());
  Assert.assertNotNull(dataSourceMap.get(dataSource));
  Assert.assertEquals(1L,dataSourceMap.get(dataSource).get());
  coordinator.stop();
  leaderUnannouncerLatch.await();
  Assert.assertFalse(coordinator.isLeader());
  Assert.assertNull(coordinator.getCurrentLeader());
  EasyMock.verify(serverInventoryView);
  EasyMock.verify(metadataRuleManager);
}
