{
  HadoopDruidIndexerConfig config=new HadoopDruidIndexerConfig(new HadoopIngestionSpec(new DataSchema("website",HadoopDruidIndexerConfig.JSON_MAPPER.convertValue(new StringInputRowParser(new TimeAndDimsParseSpec(new TimestampSpec("timestamp","yyyyMMddHH",null),new DimensionsSpec(DimensionsSpec.getDefaultSchemas(ImmutableList.of("host","keywords")),null,null)),null),Map.class),new AggregatorFactory[]{new LongSumAggregatorFactory("visited_sum","visited"),new HyperUniquesAggregatorFactory("unique_hosts","host")},new UniformGranularitySpec(Granularity.DAY,QueryGranularities.NONE,ImmutableList.of(Interval.parse("2010/2011"))),HadoopDruidIndexerConfig.JSON_MAPPER),new HadoopIOConfig(ImmutableMap.<String,Object>of("paths","/tmp/dummy","type","static"),null,"/tmp/dummy"),HadoopTuningConfig.makeDefaultTuningConfig().withWorkingPath("/tmp/work").withVersion("ver")));
  Configuration hadoopConfig=new Configuration();
  hadoopConfig.set(HadoopDruidIndexerConfig.CONFIG_PROPERTY,HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(config));
  Reducer.Context context=EasyMock.createMock(Reducer.Context.class);
  EasyMock.expect(context.getConfiguration()).andReturn(hadoopConfig);
  EasyMock.replay(context);
  aggregators=config.getSchema().getDataSchema().getAggregators();
  combiner=new IndexGeneratorJob.IndexGeneratorCombiner();
  combiner.setup(context);
}
