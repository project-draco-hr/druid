{
  final Sink theSink=new Sink(interval,schema,config,version);
  final File persistDir=new File(tmpSegmentDir,theSink.getSegment().getIdentifier());
  final Set<File> spilled=Sets.newHashSet();
  final IndexMerger theIndexMerger=config.getBuildV9Directly() ? indexMergerV9 : indexMerger;
  return new Plumber(){
    @Override public Object startJob(){
      return null;
    }
    @Override public int add(    InputRow row,    Supplier<Committer> committerSupplier) throws IndexSizeExceededException {
      Sink sink=getSink(row.getTimestampFromEpoch());
      if (sink == null) {
        return -1;
      }
      final int numRows=sink.add(row);
      if (!sink.canAppendRow()) {
        persist(committerSupplier.get());
      }
      return numRows;
    }
    private Sink getSink(    long timestamp){
      if (theSink.getInterval().contains(timestamp)) {
        return theSink;
      }
 else {
        return null;
      }
    }
    @Override public <T>QueryRunner<T> getQueryRunner(    Query<T> query){
      throw new UnsupportedOperationException("Don't query me, bro.");
    }
    @Override public void persist(    Committer committer){
      spillIfSwappable();
      committer.run();
    }
    @Override public void finishJob(){
      File fileToUpload=null;
      try {
        Preconditions.checkState(!theSink.swappable(),"All data must be persisted before fininshing the job!");
        if (spilled.size() == 0) {
          throw new IllegalStateException("Nothing indexed?");
        }
 else         if (spilled.size() == 1) {
          fileToUpload=Iterables.getOnlyElement(spilled);
        }
 else {
          List<QueryableIndex> indexes=Lists.newArrayList();
          for (          final File oneSpill : spilled) {
            indexes.add(indexIO.loadIndex(oneSpill));
          }
          fileToUpload=new File(tmpSegmentDir,"merged");
          theIndexMerger.mergeQueryableIndex(indexes,schema.getAggregators(),fileToUpload,config.getIndexSpec());
        }
        final QueryableIndex mappedSegment=indexIO.loadIndex(fileToUpload);
        final DataSegment segmentToUpload=theSink.getSegment().withDimensions(ImmutableList.copyOf(mappedSegment.getAvailableDimensions())).withBinaryVersion(SegmentUtils.getVersionFromDir(fileToUpload));
        dataSegmentPusher.push(fileToUpload,segmentToUpload);
        log.info("Uploaded segment[%s]",segmentToUpload.getIdentifier());
      }
 catch (      Exception e) {
        log.warn(e,"Failed to merge and upload");
        throw Throwables.propagate(e);
      }
 finally {
        try {
          if (fileToUpload != null) {
            log.info("Deleting Index File[%s]",fileToUpload);
            FileUtils.deleteDirectory(fileToUpload);
          }
        }
 catch (        IOException e) {
          log.warn(e,"Error deleting directory[%s]",fileToUpload);
        }
      }
    }
    private void spillIfSwappable(){
      if (theSink.swappable()) {
        final FireHydrant indexToPersist=theSink.swap();
        final int rowsToPersist=indexToPersist.getIndex().size();
        final File dirToPersist=getSpillDir(indexToPersist.getCount());
        log.info("Spilling index[%d] with rows[%d] to: %s",indexToPersist.getCount(),rowsToPersist,dirToPersist);
        try {
          theIndexMerger.persist(indexToPersist.getIndex(),dirToPersist,null,config.getIndexSpec());
          indexToPersist.swapSegment(null);
          metrics.incrementRowOutputCount(rowsToPersist);
          spilled.add(dirToPersist);
        }
 catch (        Exception e) {
          log.warn(e,"Failed to spill index[%d]",indexToPersist.getCount());
          throw Throwables.propagate(e);
        }
      }
    }
    private File getSpillDir(    final int n){
      return new File(persistDir,String.format("spill%d",n));
    }
  }
;
}
