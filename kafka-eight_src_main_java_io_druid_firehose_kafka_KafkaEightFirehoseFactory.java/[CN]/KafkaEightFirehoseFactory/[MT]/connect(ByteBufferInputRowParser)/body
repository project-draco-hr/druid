{
  Set<String> newDimExclus=Sets.union(firehoseParser.getParseSpec().getDimensionsSpec().getDimensionExclusions(),Sets.newHashSet("feed"));
  final ByteBufferInputRowParser theParser=firehoseParser.withParseSpec(firehoseParser.getParseSpec().withDimensionsSpec(firehoseParser.getParseSpec().getDimensionsSpec().withDimensionExclusions(newDimExclus)));
  final ConsumerConnector connector=Consumer.createJavaConsumerConnector(new ConsumerConfig(consumerProps));
  final Map<String,List<KafkaStream<byte[],byte[]>>> streams=connector.createMessageStreams(ImmutableMap.of(feed,1));
  final List<KafkaStream<byte[],byte[]>> streamList=streams.get(feed);
  if (streamList == null || streamList.size() != 1) {
    return null;
  }
  final KafkaStream<byte[],byte[]> stream=streamList.get(0);
  final ConsumerIterator<byte[],byte[]> iter=stream.iterator();
  return new Firehose(){
    @Override public boolean hasMore(){
      return iter.hasNext();
    }
    @Override public InputRow nextRow(){
      final byte[] message=iter.next().message();
      if (message == null) {
        return null;
      }
      try {
        return theParser.parse(ByteBuffer.wrap(message));
      }
 catch (      Exception e) {
        log.error("Unparseable row! Error parsing[%s]",ByteBuffer.wrap(message));
        throw Throwables.propagate(e);
      }
    }
    @Override public Runnable commit(){
      return new Runnable(){
        @Override public void run(){
          log.info("committing offsets");
          connector.commitOffsets();
        }
      }
;
    }
    @Override public void close() throws IOException {
      connector.shutdown();
    }
  }
;
}
