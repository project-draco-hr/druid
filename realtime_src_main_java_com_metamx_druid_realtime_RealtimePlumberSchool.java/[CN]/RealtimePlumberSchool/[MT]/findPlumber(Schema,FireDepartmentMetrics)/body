{
  verifyState();
  initializeExecutors();
  computeBaseDir(schema).mkdirs();
  final Map<Long,Sink> sinks=Maps.newConcurrentMap();
  for (  File sinkDir : computeBaseDir(schema).listFiles()) {
    Interval sinkInterval=new Interval(sinkDir.getName().replace("_","/"));
    final File[] sinkFiles=sinkDir.listFiles();
    Arrays.sort(sinkFiles,new Comparator<File>(){
      @Override public int compare(      File o1,      File o2){
        try {
          return Ints.compare(Integer.parseInt(o1.getName()),Integer.parseInt(o2.getName()));
        }
 catch (        NumberFormatException e) {
          log.error(e,"Couldn't compare as numbers? [%s][%s]",o1,o2);
          return o1.compareTo(o2);
        }
      }
    }
);
    try {
      List<FireHydrant> hydrants=Lists.newArrayList();
      for (      File segmentDir : sinkFiles) {
        log.info("Loading previously persisted segment at [%s]",segmentDir);
        hydrants.add(new FireHydrant(new QueryableIndexSegment(null,IndexIO.loadIndex(segmentDir)),Integer.parseInt(segmentDir.getName())));
      }
      Sink currSink=new Sink(sinkInterval,schema,hydrants);
      sinks.put(sinkInterval.getStartMillis(),currSink);
      metadataUpdater.announceSegment(currSink.getSegment());
    }
 catch (    IOException e) {
      log.makeAlert(e,"Problem loading sink[%s] from disk.",schema.getDataSource()).addData("interval",sinkInterval).emit();
    }
  }
  serverView.registerSegmentCallback(persistExecutor,new ServerView.BaseSegmentCallback(){
    @Override public ServerView.CallbackAction segmentAdded(    DruidServer server,    DataSegment segment){
      if ("realtime".equals(server.getType())) {
        return ServerView.CallbackAction.CONTINUE;
      }
      log.debug("Checking segment[%s] on server[%s]",segment,server);
      if (schema.getDataSource().equals(segment.getDataSource())) {
        final Interval interval=segment.getInterval();
        for (        Map.Entry<Long,Sink> entry : sinks.entrySet()) {
          final Long sinkKey=entry.getKey();
          if (interval.contains(sinkKey)) {
            final Sink sink=entry.getValue();
            log.info("Segment matches sink[%s]",sink);
            if (segment.getVersion().compareTo(sink.getSegment().getVersion()) >= 0) {
              try {
                metadataUpdater.unannounceSegment(sink.getSegment());
                FileUtils.deleteDirectory(computePersistDir(schema,sink.getInterval()));
                sinks.remove(sinkKey);
              }
 catch (              IOException e) {
                log.makeAlert(e,"Unable to delete old segment for dataSource[%s].",schema.getDataSource()).addData("interval",sink.getInterval()).emit();
              }
            }
          }
        }
      }
      return ServerView.CallbackAction.CONTINUE;
    }
  }
);
  final long truncatedNow=segmentGranularity.truncate(new DateTime()).getMillis();
  final long windowMillis=windowPeriod.toStandardDuration().getMillis();
  final RejectionPolicy rejectionPolicy=rejectionPolicyFactory.create(windowPeriod);
  log.info("Creating plumber using rejectionPolicy[%s]",rejectionPolicy);
  log.info("Expect to run at [%s]",new DateTime().plus(new Duration(System.currentTimeMillis(),segmentGranularity.increment(truncatedNow) + windowMillis)));
  ScheduledExecutors.scheduleAtFixedRate(scheduledExecutor,new Duration(System.currentTimeMillis(),segmentGranularity.increment(truncatedNow) + windowMillis),new Duration(truncatedNow,segmentGranularity.increment(truncatedNow)),new ThreadRenamingRunnable(String.format("%s-overseer",schema.getDataSource())){
    @Override public void doRun(){
      log.info("Starting merge and push.");
      long minTimestamp=segmentGranularity.truncate(rejectionPolicy.getCurrMaxTime()).getMillis() - windowMillis;
      List<Map.Entry<Long,Sink>> sinksToPush=Lists.newArrayList();
      for (      Map.Entry<Long,Sink> entry : sinks.entrySet()) {
        final Long intervalStart=entry.getKey();
        if (intervalStart < minTimestamp) {
          log.info("Adding entry[%s] for merge and push.",entry);
          sinksToPush.add(entry);
        }
      }
      for (      final Map.Entry<Long,Sink> entry : sinksToPush) {
        final Sink sink=entry.getValue();
        final String threadName=String.format("%s-%s-persist-n-merge",schema.getDataSource(),new DateTime(entry.getKey()));
        persistExecutor.execute(new ThreadRenamingRunnable(threadName){
          @Override public void doRun(){
            final Interval interval=sink.getInterval();
            for (            FireHydrant hydrant : sink) {
              if (!hydrant.hasSwapped()) {
                log.info("Hydrant[%s] hasn't swapped yet, swapping. Sink[%s]",hydrant,sink);
                final int rowCount=persistHydrant(hydrant,schema,interval);
                metrics.incrementRowOutputCount(rowCount);
              }
            }
            final File mergedFile;
            try {
              List<QueryableIndex> indexes=Lists.newArrayList();
              for (              FireHydrant fireHydrant : sink) {
                Segment segment=fireHydrant.getSegment();
                final QueryableIndex queryableIndex=segment.asQueryableIndex();
                log.info("Adding hydrant[%s]",fireHydrant);
                indexes.add(queryableIndex);
              }
              mergedFile=IndexMerger.mergeQueryableIndex(indexes,schema.getAggregators(),new File(computePersistDir(schema,interval),"merged"));
              QueryableIndex index=IndexIO.loadIndex(mergedFile);
              DataSegment segment=dataSegmentPusher.push(mergedFile,sink.getSegment().withDimensions(Lists.newArrayList(index.getAvailableDimensions())));
              metadataUpdater.publishSegment(segment);
            }
 catch (            IOException e) {
              log.makeAlert(e,"Failed to persist merged index[%s]",schema.getDataSource()).addData("interval",interval).emit();
            }
          }
        }
);
      }
    }
  }
);
  return new Plumber(){
    @Override public Sink getSink(    long timestamp){
      if (!rejectionPolicy.accept(timestamp)) {
        return null;
      }
      final long truncatedTime=segmentGranularity.truncate(timestamp);
      Sink retVal=sinks.get(truncatedTime);
      if (retVal == null) {
        retVal=new Sink(new Interval(new DateTime(truncatedTime),segmentGranularity.increment(new DateTime(truncatedTime))),schema);
        try {
          metadataUpdater.announceSegment(retVal.getSegment());
          sinks.put(truncatedTime,retVal);
        }
 catch (        IOException e) {
          log.makeAlert(e,"Failed to announce new segment[%s]",schema.getDataSource()).addData("interval",retVal.getInterval()).emit();
        }
      }
      return retVal;
    }
    @Override public <T>QueryRunner<T> getQueryRunner(    final Query<T> query){
      final QueryRunnerFactory<T,Query<T>> factory=conglomerate.findFactory(query);
      final Function<Query<T>,ServiceMetricEvent.Builder> builderFn=new Function<Query<T>,ServiceMetricEvent.Builder>(){
        private final QueryToolChest<T,Query<T>> toolchest=factory.getToolchest();
        @Override public ServiceMetricEvent.Builder apply(        @Nullable Query<T> input){
          return toolchest.makeMetricBuilder(query);
        }
      }
;
      return factory.mergeRunners(EXEC,FunctionalIterable.create(sinks.values()).transform(new Function<Sink,QueryRunner<T>>(){
        @Override public QueryRunner<T> apply(        @Nullable Sink input){
          return new MetricsEmittingQueryRunner<T>(emitter,builderFn,factory.mergeRunners(EXEC,Iterables.transform(input,new Function<FireHydrant,QueryRunner<T>>(){
            @Override public QueryRunner<T> apply(            @Nullable FireHydrant input){
              return factory.createRunner(input.getSegment());
            }
          }
)));
        }
      }
));
    }
    @Override public void persist(    final Runnable commitRunnable){
      final List<Pair<FireHydrant,Interval>> indexesToPersist=Lists.newArrayList();
      for (      Sink sink : sinks.values()) {
        if (sink.swappable()) {
          indexesToPersist.add(Pair.of(sink.swap(),sink.getInterval()));
        }
      }
      log.info("Submitting persist runnable for dataSource[%s]",schema.getDataSource());
      persistExecutor.execute(new ThreadRenamingRunnable(String.format("%s-incremental-persist",schema.getDataSource())){
        @Override public void doRun(){
          for (          Pair<FireHydrant,Interval> pair : indexesToPersist) {
            metrics.incrementRowOutputCount(persistHydrant(pair.lhs,schema,pair.rhs));
          }
          commitRunnable.run();
        }
      }
);
    }
    @Override public void finishJob(){
      throw new UnsupportedOperationException();
    }
  }
;
}
