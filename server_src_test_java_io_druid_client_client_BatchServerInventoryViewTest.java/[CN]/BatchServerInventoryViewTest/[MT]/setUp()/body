{
  testingCluster=new TestingCluster(1);
  testingCluster.start();
  cf=CuratorFrameworkFactory.builder().connectString(testingCluster.getConnectString()).retryPolicy(new ExponentialBackoffRetry(1,10)).compressionProvider(new PotentiallyGzippedCompressionProvider(true)).build();
  cf.start();
  cf.create().creatingParentsIfNeeded().forPath(testBasePath);
  jsonMapper=new DefaultObjectMapper();
  announcer=new Announcer(cf,MoreExecutors.sameThreadExecutor());
  announcer.start();
  segmentAnnouncer=new BatchDataSegmentAnnouncer(new DruidServerMetadata("id","host",Long.MAX_VALUE,"type","tier",0),new BatchDataSegmentAnnouncerConfig(){
    @Override public int getSegmentsPerNode(){
      return 50;
    }
  }
,new ZkPathsConfig(){
    @Override public String getBase(){
      return testBasePath;
    }
  }
,announcer,jsonMapper);
  segmentAnnouncer.start();
  testSegments=Sets.newConcurrentHashSet();
  for (int i=0; i < INITIAL_SEGMENTS; i++) {
    testSegments.add(makeSegment(i));
  }
  batchServerInventoryView=new BatchServerInventoryView(new ZkPathsConfig(){
    @Override public String getBase(){
      return testBasePath;
    }
  }
,cf,jsonMapper,Predicates.<DataSegment>alwaysTrue());
  batchServerInventoryView.start();
  inventoryUpdateCounter.set(0);
  filteredBatchServerInventoryView=new BatchServerInventoryView(new ZkPathsConfig(){
    @Override public String getBase(){
      return testBasePath;
    }
  }
,cf,jsonMapper,new Predicate<DataSegment>(){
    @Override public boolean apply(    @Nullable DataSegment dataSegment){
      return dataSegment.getInterval().getStart().isBefore(SEGMENT_INTERVAL_START.plusDays(INITIAL_SEGMENTS));
    }
  }
){
    @Override protected DruidServer addInnerInventory(    DruidServer container,    String inventoryKey,    Set<DataSegment> inventory){
      DruidServer server=super.addInnerInventory(container,inventoryKey,inventory);
      inventoryUpdateCounter.incrementAndGet();
      return server;
    }
  }
;
  filteredBatchServerInventoryView.start();
}
