{
  verifyState();
  final RejectionPolicy rejectionPolicy=rejectionPolicyFactory.create(windowPeriod);
  log.info("Creating plumber using rejectionPolicy[%s]",rejectionPolicy);
  return new Plumber(){
    private volatile boolean stopped=false;
    private volatile ExecutorService persistExecutor=null;
    private volatile ScheduledExecutorService scheduledExecutor=null;
    private final Map<Long,Sink> sinks=Maps.newConcurrentMap();
    private final VersionedIntervalTimeline<String,Sink> sinkTimeline=new VersionedIntervalTimeline<String,Sink>(String.CASE_INSENSITIVE_ORDER);
    @Override public void startJob(){
      computeBaseDir(schema).mkdirs();
      initializeExecutors();
      bootstrapSinksFromDisk();
      registerServerViewCallback();
      startPersistThread();
    }
    @Override public Sink getSink(    long timestamp){
      if (!rejectionPolicy.accept(timestamp)) {
        return null;
      }
      final long truncatedTime=segmentGranularity.truncate(timestamp);
      Sink retVal=sinks.get(truncatedTime);
      if (retVal == null) {
        final Interval sinkInterval=new Interval(new DateTime(truncatedTime),segmentGranularity.increment(new DateTime(truncatedTime)));
        retVal=new Sink(sinkInterval,schema,versioningPolicy.getVersion(sinkInterval));
        try {
          segmentAnnouncer.announceSegment(retVal.getSegment());
          sinks.put(truncatedTime,retVal);
          sinkTimeline.add(retVal.getInterval(),retVal.getVersion(),new SingleElementPartitionChunk<Sink>(retVal));
        }
 catch (        IOException e) {
          log.makeAlert(e,"Failed to announce new segment[%s]",schema.getDataSource()).addData("interval",retVal.getInterval()).emit();
        }
      }
      return retVal;
    }
    @Override public <T>QueryRunner<T> getQueryRunner(    final Query<T> query){
      final QueryRunnerFactory<T,Query<T>> factory=conglomerate.findFactory(query);
      final QueryToolChest<T,Query<T>> toolchest=factory.getToolchest();
      final Function<Query<T>,ServiceMetricEvent.Builder> builderFn=new Function<Query<T>,ServiceMetricEvent.Builder>(){
        @Override public ServiceMetricEvent.Builder apply(        @Nullable Query<T> input){
          return toolchest.makeMetricBuilder(query);
        }
      }
;
      List<TimelineObjectHolder<String,Sink>> querySinks=Lists.newArrayList();
      for (      Interval interval : query.getIntervals()) {
        querySinks.addAll(sinkTimeline.lookup(interval));
      }
      return toolchest.mergeResults(factory.mergeRunners(queryExecutorService,FunctionalIterable.create(querySinks).transform(new Function<TimelineObjectHolder<String,Sink>,QueryRunner<T>>(){
        @Override public QueryRunner<T> apply(        TimelineObjectHolder<String,Sink> holder){
          final Sink theSink=holder.getObject().getChunk(0).getObject();
          return new SpecificSegmentQueryRunner<T>(new MetricsEmittingQueryRunner<T>(emitter,builderFn,factory.mergeRunners(MoreExecutors.sameThreadExecutor(),Iterables.transform(theSink,new Function<FireHydrant,QueryRunner<T>>(){
            @Override public QueryRunner<T> apply(            FireHydrant input){
              return factory.createRunner(input.getSegment());
            }
          }
))),new SpecificSegmentSpec(new SegmentDescriptor(holder.getInterval(),theSink.getSegment().getVersion(),theSink.getSegment().getShardSpec().getPartitionNum())));
        }
      }
)));
    }
    @Override public void persist(    final Runnable commitRunnable){
      final List<Pair<FireHydrant,Interval>> indexesToPersist=Lists.newArrayList();
      for (      Sink sink : sinks.values()) {
        if (sink.swappable()) {
          indexesToPersist.add(Pair.of(sink.swap(),sink.getInterval()));
        }
      }
      log.info("Submitting persist runnable for dataSource[%s]",schema.getDataSource());
      persistExecutor.execute(new ThreadRenamingRunnable(String.format("%s-incremental-persist",schema.getDataSource())){
        @Override public void doRun(){
          for (          Pair<FireHydrant,Interval> pair : indexesToPersist) {
            metrics.incrementRowOutputCount(persistHydrant(pair.lhs,schema,pair.rhs));
          }
          commitRunnable.run();
        }
      }
);
    }
    private void persistAndMerge(    final long truncatedTime,    final Sink sink){
      final String threadName=String.format("%s-%s-persist-n-merge",schema.getDataSource(),new DateTime(truncatedTime));
      persistExecutor.execute(new ThreadRenamingRunnable(threadName){
        @Override public void doRun(){
          final Interval interval=sink.getInterval();
          for (          FireHydrant hydrant : sink) {
            if (!hydrant.hasSwapped()) {
              log.info("Hydrant[%s] hasn't swapped yet, swapping. Sink[%s]",hydrant,sink);
              final int rowCount=persistHydrant(hydrant,schema,interval);
              metrics.incrementRowOutputCount(rowCount);
            }
          }
          final File mergedTarget=new File(computePersistDir(schema,interval),"merged");
          if (mergedTarget.exists()) {
            log.info("Skipping already-merged sink: %s",sink);
            return;
          }
          File mergedFile=null;
          try {
            List<QueryableIndex> indexes=Lists.newArrayList();
            for (            FireHydrant fireHydrant : sink) {
              Segment segment=fireHydrant.getSegment();
              final QueryableIndex queryableIndex=segment.asQueryableIndex();
              log.info("Adding hydrant[%s]",fireHydrant);
              indexes.add(queryableIndex);
            }
            mergedFile=IndexMerger.mergeQueryableIndex(indexes,schema.getAggregators(),mergedTarget);
            QueryableIndex index=IndexIO.loadIndex(mergedFile);
            DataSegment segment=dataSegmentPusher.push(mergedFile,sink.getSegment().withDimensions(Lists.newArrayList(index.getAvailableDimensions())));
            segmentPublisher.publishSegment(segment);
          }
 catch (          IOException e) {
            log.makeAlert(e,"Failed to persist merged index[%s]",schema.getDataSource()).addData("interval",interval).emit();
          }
          if (mergedFile != null) {
            try {
              if (mergedFile != null) {
                log.info("Deleting Index File[%s]",mergedFile);
                FileUtils.deleteDirectory(mergedFile);
              }
            }
 catch (            IOException e) {
              log.warn(e,"Error deleting directory[%s]",mergedFile);
            }
          }
        }
      }
);
    }
    @Override public void finishJob(){
      log.info("Shutting down...");
      for (      final Map.Entry<Long,Sink> entry : sinks.entrySet()) {
        persistAndMerge(entry.getKey(),entry.getValue());
      }
      while (!sinks.isEmpty()) {
        try {
          log.info("Cannot shut down yet! Sinks remaining: %s",Joiner.on(", ").join(Iterables.transform(sinks.values(),new Function<Sink,String>(){
            @Override public String apply(            Sink input){
              return input.getSegment().getIdentifier();
            }
          }
)));
synchronized (handoffCondition) {
            while (!sinks.isEmpty()) {
              handoffCondition.wait();
            }
          }
        }
 catch (        InterruptedException e) {
          throw Throwables.propagate(e);
        }
      }
      if (scheduledExecutor != null) {
        scheduledExecutor.shutdown();
      }
      stopped=true;
    }
    private void initializeExecutors(){
      if (persistExecutor == null) {
        persistExecutor=Executors.newFixedThreadPool(1,new ThreadFactoryBuilder().setDaemon(true).setNameFormat("plumber_persist_%d").build());
      }
      if (scheduledExecutor == null) {
        scheduledExecutor=Executors.newScheduledThreadPool(1,new ThreadFactoryBuilder().setDaemon(true).setNameFormat("plumber_scheduled_%d").build());
      }
    }
    private void bootstrapSinksFromDisk(){
      for (      File sinkDir : computeBaseDir(schema).listFiles()) {
        Interval sinkInterval=new Interval(sinkDir.getName().replace("_","/"));
        final File[] sinkFiles=sinkDir.listFiles(new FilenameFilter(){
          @Override public boolean accept(          File dir,          String fileName){
            return !(Ints.tryParse(fileName) == null);
          }
        }
);
        Arrays.sort(sinkFiles,new Comparator<File>(){
          @Override public int compare(          File o1,          File o2){
            try {
              return Ints.compare(Integer.parseInt(o1.getName()),Integer.parseInt(o2.getName()));
            }
 catch (            NumberFormatException e) {
              log.error(e,"Couldn't compare as numbers? [%s][%s]",o1,o2);
              return o1.compareTo(o2);
            }
          }
        }
);
        try {
          List<FireHydrant> hydrants=Lists.newArrayList();
          for (          File segmentDir : sinkFiles) {
            log.info("Loading previously persisted segment at [%s]",segmentDir);
            if (Ints.tryParse(segmentDir.getName()) == null) {
              continue;
            }
            hydrants.add(new FireHydrant(new QueryableIndexSegment(null,IndexIO.loadIndex(segmentDir)),Integer.parseInt(segmentDir.getName())));
          }
          Sink currSink=new Sink(sinkInterval,schema,versioningPolicy.getVersion(sinkInterval),hydrants);
          sinks.put(sinkInterval.getStartMillis(),currSink);
          sinkTimeline.add(currSink.getInterval(),currSink.getVersion(),new SingleElementPartitionChunk<Sink>(currSink));
          segmentAnnouncer.announceSegment(currSink.getSegment());
        }
 catch (        IOException e) {
          log.makeAlert(e,"Problem loading sink[%s] from disk.",schema.getDataSource()).addData("interval",sinkInterval).emit();
        }
      }
    }
    private void registerServerViewCallback(){
      serverView.registerSegmentCallback(persistExecutor,new ServerView.BaseSegmentCallback(){
        @Override public ServerView.CallbackAction segmentAdded(        DruidServer server,        DataSegment segment){
          if (stopped) {
            log.info("Unregistering ServerViewCallback");
            persistExecutor.shutdown();
            return ServerView.CallbackAction.UNREGISTER;
          }
          if ("realtime".equals(server.getType())) {
            return ServerView.CallbackAction.CONTINUE;
          }
          log.debug("Checking segment[%s] on server[%s]",segment,server);
          if (schema.getDataSource().equals(segment.getDataSource())) {
            final Interval interval=segment.getInterval();
            for (            Map.Entry<Long,Sink> entry : sinks.entrySet()) {
              final Long sinkKey=entry.getKey();
              if (interval.contains(sinkKey)) {
                final Sink sink=entry.getValue();
                log.info("Segment[%s] matches sink[%s] on server[%s]",segment,sink,server);
                final String segmentVersion=segment.getVersion();
                final String sinkVersion=sink.getSegment().getVersion();
                if (segmentVersion.compareTo(sinkVersion) >= 0) {
                  log.info("Segment version[%s] >= sink version[%s]",segmentVersion,sinkVersion);
                  try {
                    segmentAnnouncer.unannounceSegment(sink.getSegment());
                    FileUtils.deleteDirectory(computePersistDir(schema,sink.getInterval()));
                    log.info("Removing sinkKey %d for segment %s",sinkKey,sink.getSegment().getIdentifier());
                    sinks.remove(sinkKey);
                    sinkTimeline.remove(sink.getInterval(),sink.getVersion(),new SingleElementPartitionChunk<Sink>(sink));
synchronized (handoffCondition) {
                      handoffCondition.notifyAll();
                    }
                  }
 catch (                  IOException e) {
                    log.makeAlert(e,"Unable to delete old segment for dataSource[%s].",schema.getDataSource()).addData("interval",sink.getInterval()).emit();
                  }
                }
              }
            }
          }
          return ServerView.CallbackAction.CONTINUE;
        }
      }
);
    }
    private void startPersistThread(){
      final long truncatedNow=segmentGranularity.truncate(new DateTime()).getMillis();
      final long windowMillis=windowPeriod.toStandardDuration().getMillis();
      log.info("Expect to run at [%s]",new DateTime().plus(new Duration(System.currentTimeMillis(),segmentGranularity.increment(truncatedNow) + windowMillis)));
      ScheduledExecutors.scheduleAtFixedRate(scheduledExecutor,new Duration(System.currentTimeMillis(),segmentGranularity.increment(truncatedNow) + windowMillis),new Duration(truncatedNow,segmentGranularity.increment(truncatedNow)),new ThreadRenamingCallable<ScheduledExecutors.Signal>(String.format("%s-overseer-%d",schema.getDataSource(),schema.getShardSpec().getPartitionNum())){
        @Override public ScheduledExecutors.Signal doCall(){
          if (stopped) {
            log.info("Stopping merge-n-push overseer thread");
            return ScheduledExecutors.Signal.STOP;
          }
          log.info("Starting merge and push.");
          long minTimestamp=segmentGranularity.truncate(rejectionPolicy.getCurrMaxTime().minus(windowMillis)).getMillis();
          List<Map.Entry<Long,Sink>> sinksToPush=Lists.newArrayList();
          for (          Map.Entry<Long,Sink> entry : sinks.entrySet()) {
            final Long intervalStart=entry.getKey();
            if (intervalStart < minTimestamp) {
              log.info("Adding entry[%s] for merge and push.",entry);
              sinksToPush.add(entry);
            }
          }
          for (          final Map.Entry<Long,Sink> entry : sinksToPush) {
            persistAndMerge(entry.getKey(),entry.getValue());
          }
          if (stopped) {
            log.info("Stopping merge-n-push overseer thread");
            return ScheduledExecutors.Signal.STOP;
          }
 else {
            return ScheduledExecutors.Signal.REPEAT;
          }
        }
      }
);
    }
  }
;
}
