{
  emitter=EasyMock.createMock(ServiceEmitter.class);
  EmittingLogger.registerEmitter(emitter);
  queryRunnerFactoryConglomerate=EasyMock.createStrictMock(QueryRunnerFactoryConglomerate.class);
  monitorScheduler=EasyMock.createStrictMock(MonitorScheduler.class);
  publishCountDown=new CountDownLatch(1);
  announcedSinks=0;
  pushedSegments=0;
  tmpDir=temporaryFolder.newFolder();
  TestUtils testUtils=new TestUtils();
  mapper=testUtils.getTestObjectMapper();
  tqc=mapper.readValue("{\"startDelay\":\"PT0S\", \"restartDelay\":\"PT1S\", \"storageSyncRate\":\"PT0.5S\"}",TaskQueueConfig.class);
  indexSpec=new IndexSpec();
  if (taskStorageType.equals("HeapMemoryTaskStorage")) {
    ts=new HeapMemoryTaskStorage(new TaskStorageConfig(null){
    }
);
  }
 else   if (taskStorageType.equals("MetadataTaskStorage")) {
    testDerbyConnector=derbyConnectorRule.getConnector();
    mapper.registerSubtypes(new NamedType(MockExceptionalFirehoseFactory.class,"mockExcepFirehoseFactory"),new NamedType(MockFirehoseFactory.class,"mockFirehoseFactory"));
    testDerbyConnector.createTaskTables();
    testDerbyConnector.createSegmentTable();
    ts=new MetadataTaskStorage(testDerbyConnector,new TaskStorageConfig(null),new SQLMetadataStorageActionHandlerFactory(testDerbyConnector,derbyConnectorRule.metadataTablesConfigSupplier().get(),mapper));
  }
 else {
    throw new RuntimeException(String.format("Unknown task storage type [%s]",taskStorageType));
  }
  serverView=new FilteredServerView(){
    @Override public void registerSegmentCallback(    Executor exec,    ServerView.SegmentCallback callback,    Predicate<DataSegment> filter){
      segmentCallbacks.add(callback);
    }
  }
;
  setUpAndStartTaskQueue(new DataSegmentPusher(){
    @Override public String getPathForHadoop(    String dataSource){
      throw new UnsupportedOperationException();
    }
    @Override public DataSegment push(    File file,    DataSegment segment) throws IOException {
      pushedSegments++;
      return segment;
    }
  }
);
}
