{
  this.expectedNumOfShards=expectedNumOfShards;
  this.expectedNumTimeBuckets=expectedNumTimeBuckets;
  this.errorMargin=errorMargin;
  File tmpDir=Files.createTempDir();
  tmpDir.deleteOnExit();
  HadoopIngestionSpec ingestionSpec=new HadoopIngestionSpec(new DataSchema("test_schema",HadoopDruidIndexerConfig.JSON_MAPPER.convertValue(new StringInputRowParser(new DelimitedParseSpec(new TimestampSpec("ts",null,null),new DimensionsSpec(DimensionsSpec.getDefaultSchemas(ImmutableList.of("market","quality","placement","placementish")),null,null),"\t",null,Arrays.asList("ts","market","quality","placement","placementish","index")),null),Map.class),new AggregatorFactory[]{new DoubleSumAggregatorFactory("index","index")},new UniformGranularitySpec(Granularity.DAY,QueryGranularities.NONE,ImmutableList.of(new Interval(interval))),HadoopDruidIndexerConfig.JSON_MAPPER),new HadoopIOConfig(ImmutableMap.<String,Object>of("paths",dataFilePath,"type","static"),null,tmpDir.getAbsolutePath()),new HadoopTuningConfig(tmpDir.getAbsolutePath(),null,new HashedPartitionsSpec(targetPartitionSize,null,true,null,null),null,null,null,false,false,false,false,null,false,false,null,null,null));
  this.indexerConfig=new HadoopDruidIndexerConfig(ingestionSpec);
}
