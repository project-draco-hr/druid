{
  Configuration conf=context.getConfiguration();
  String segmentsStr=Preconditions.checkNotNull(conf.get(CONF_INPUT_SEGMENTS),"No segments found to read");
  List<WindowedDataSegment> segments=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(segmentsStr,new TypeReference<List<WindowedDataSegment>>(){
  }
);
  if (segments == null || segments.size() == 0) {
    throw new ISE("No segments found to read");
  }
  logger.info("segments to read [%s]",segmentsStr);
  long maxSize=conf.getLong(CONF_MAX_SPLIT_SIZE,0);
  if (maxSize < 0) {
    long totalSize=0;
    for (    WindowedDataSegment segment : segments) {
      totalSize+=segment.getSegment().getSize();
    }
    int mapTask=((JobConf)conf).getNumMapTasks();
    if (mapTask > 0) {
      maxSize=totalSize / mapTask;
    }
  }
  if (maxSize > 0) {
    Collections.sort(segments,new Comparator<WindowedDataSegment>(){
      @Override public int compare(      WindowedDataSegment s1,      WindowedDataSegment s2){
        return Long.compare(s1.getSegment().getSize(),s2.getSegment().getSize());
      }
    }
);
  }
  List<InputSplit> splits=Lists.newArrayList();
  List<WindowedDataSegment> list=new ArrayList<>();
  long size=0;
  JobConf dummyConf=new JobConf();
  org.apache.hadoop.mapred.InputFormat fio=supplier.get();
  for (  WindowedDataSegment segment : segments) {
    if (size + segment.getSegment().getSize() > maxSize && size > 0) {
      splits.add(toDataSourceSplit(list,fio,dummyConf));
      list=Lists.newArrayList();
      size=0;
    }
    list.add(segment);
    size+=segment.getSegment().getSize();
  }
  if (list.size() > 0) {
    splits.add(toDataSourceSplit(list,fio,dummyConf));
  }
  logger.info("Number of splits [%d]",splits.size());
  return splits;
}
