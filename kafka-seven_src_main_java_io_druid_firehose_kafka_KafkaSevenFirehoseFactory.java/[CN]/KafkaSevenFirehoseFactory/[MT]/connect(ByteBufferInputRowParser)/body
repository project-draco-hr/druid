{
  Set<String> newDimExclus=Sets.union(firehoseParser.getParseSpec().getDimensionsSpec().getDimensionExclusions(),Sets.newHashSet("feed"));
  final ByteBufferInputRowParser theParser=firehoseParser.withParseSpec(firehoseParser.getParseSpec().withDimensionsSpec(firehoseParser.getParseSpec().getDimensionsSpec().withDimensionExclusions(newDimExclus)));
  final ConsumerConnector connector=Consumer.createJavaConsumerConnector(new ConsumerConfig(consumerProps));
  final Map<String,List<KafkaStream<Message>>> streams=connector.createMessageStreams(ImmutableMap.of(feed,1));
  final List<KafkaStream<Message>> streamList=streams.get(feed);
  if (streamList == null || streamList.size() != 1) {
    return null;
  }
  final KafkaStream<Message> stream=streamList.get(0);
  final Iterator<MessageAndMetadata<Message>> iter=stream.iterator();
  return new Firehose(){
    @Override public boolean hasMore(){
      return iter.hasNext();
    }
    @Override public InputRow nextRow(){
      final Message message=iter.next().message();
      if (message == null) {
        return null;
      }
      return parseMessage(message);
    }
    public InputRow parseMessage(    Message message){
      return theParser.parse(message.payload());
    }
    @Override public Runnable commit(){
      return new Runnable(){
        @Override public void run(){
          log.info("committing offsets");
          connector.commitOffsets();
        }
      }
;
    }
    @Override public void close() throws IOException {
      connector.shutdown();
    }
  }
;
}
