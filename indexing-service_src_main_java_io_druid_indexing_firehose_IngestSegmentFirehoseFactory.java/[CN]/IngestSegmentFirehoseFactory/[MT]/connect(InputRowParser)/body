{
  log.info("Connecting firehose: dataSource[%s], interval[%s]",dataSource,interval);
  final TaskToolbox toolbox=injector.getInstance(TaskToolboxFactory.class).build(new NoopTask("reingest",0,0,null,null));
  try {
    final List<DataSegment> usedSegments=toolbox.getTaskActionClient().submit(new SegmentListUsedAction(dataSource,interval));
    final Map<DataSegment,File> segmentFileMap=toolbox.fetchSegments(usedSegments);
    VersionedIntervalTimeline<String,DataSegment> timeline=new VersionedIntervalTimeline<String,DataSegment>(Ordering.<String>natural().nullsFirst());
    for (    DataSegment segment : usedSegments) {
      timeline.add(segment.getInterval(),segment.getVersion(),segment.getShardSpec().createChunk(segment));
    }
    final List<TimelineObjectHolder<String,DataSegment>> timeLineSegments=timeline.lookup(interval);
    List<String> dims;
    if (dimensions != null) {
      dims=dimensions;
    }
 else     if (inputRowParser.getParseSpec().getDimensionsSpec().hasCustomDimensions()) {
      dims=inputRowParser.getParseSpec().getDimensionsSpec().getDimensions();
    }
 else {
      Set<String> dimSet=Sets.newHashSet(Iterables.concat(Iterables.transform(timeLineSegments,new Function<TimelineObjectHolder<String,DataSegment>,Iterable<String>>(){
        @Override public Iterable<String> apply(        TimelineObjectHolder<String,DataSegment> timelineObjectHolder){
          return Iterables.concat(Iterables.transform(timelineObjectHolder.getObject(),new Function<PartitionChunk<DataSegment>,Iterable<String>>(){
            @Override public Iterable<String> apply(            PartitionChunk<DataSegment> input){
              return input.getObject().getDimensions();
            }
          }
));
        }
      }
)));
      dims=Lists.newArrayList(Sets.difference(dimSet,inputRowParser.getParseSpec().getDimensionsSpec().getDimensionExclusions()));
    }
    List<String> metricsList;
    if (metrics != null) {
      metricsList=metrics;
    }
 else {
      Set<String> metricsSet=Sets.newHashSet(Iterables.concat(Iterables.transform(timeLineSegments,new Function<TimelineObjectHolder<String,DataSegment>,Iterable<String>>(){
        @Override public Iterable<String> apply(        TimelineObjectHolder<String,DataSegment> input){
          return Iterables.concat(Iterables.transform(input.getObject(),new Function<PartitionChunk<DataSegment>,Iterable<String>>(){
            @Override public Iterable<String> apply(            PartitionChunk<DataSegment> input){
              return input.getObject().getMetrics();
            }
          }
));
        }
      }
)));
      metricsList=Lists.newArrayList(metricsSet);
    }
    final List<StorageAdapter> adapters=Lists.newArrayList(Iterables.concat(Iterables.transform(timeLineSegments,new Function<TimelineObjectHolder<String,DataSegment>,Iterable<StorageAdapter>>(){
      @Override public Iterable<StorageAdapter> apply(      TimelineObjectHolder<String,DataSegment> input){
        return Iterables.transform(input.getObject(),new Function<PartitionChunk<DataSegment>,StorageAdapter>(){
          @Override public StorageAdapter apply(          PartitionChunk<DataSegment> input){
            final DataSegment segment=input.getObject();
            try {
              return new QueryableIndexStorageAdapter(IndexIO.loadIndex(Preconditions.checkNotNull(segmentFileMap.get(segment),"File for segment %s",segment.getIdentifier())));
            }
 catch (            IOException e) {
              throw Throwables.propagate(e);
            }
          }
        }
);
      }
    }
)));
    return new IngestSegmentFirehose(adapters,dims,metricsList,dimFilter,interval,QueryGranularity.NONE);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
catch (  SegmentLoadingException e) {
    throw Throwables.propagate(e);
  }
}
