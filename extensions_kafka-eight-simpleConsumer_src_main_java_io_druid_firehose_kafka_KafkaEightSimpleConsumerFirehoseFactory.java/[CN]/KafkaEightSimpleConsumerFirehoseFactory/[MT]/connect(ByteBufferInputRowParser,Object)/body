{
  final Map<Integer,Long> lastOffsets=loadOffsetFromPreviousMetaData(lastCommit);
  Set<String> newDimExclus=Sets.union(firehoseParser.getParseSpec().getDimensionsSpec().getDimensionExclusions(),Sets.newHashSet("feed"));
  final ByteBufferInputRowParser theParser=firehoseParser.withParseSpec(firehoseParser.getParseSpec().withDimensionsSpec(firehoseParser.getParseSpec().getDimensionsSpec().withDimensionExclusions(newDimExclus)));
  for (  Integer partition : partitionIdList) {
    final KafkaSimpleConsumer kafkaSimpleConsumer=new KafkaSimpleConsumer(feed,partition,clientId,brokerList,earliest);
    Long startOffset=lastOffsets.get(partition);
    PartitionConsumerWorker worker=new PartitionConsumerWorker(feed,kafkaSimpleConsumer,partition,startOffset == null ? 0 : startOffset);
    consumerWorkers.add(worker);
  }
  final LinkedBlockingQueue<BytesMessageWithOffset> messageQueue=new LinkedBlockingQueue<BytesMessageWithOffset>(queueBufferLength);
  log.info("Kicking off all consumers");
  for (  PartitionConsumerWorker worker : consumerWorkers) {
    worker.go(messageQueue);
  }
  log.info("All consumer started");
  return new FirehoseV2(){
    private ConcurrentMap<Integer,Long> lastOffsetPartitions;
    private volatile boolean stop;
    private volatile boolean interrupted;
    private volatile BytesMessageWithOffset msg=null;
    private volatile InputRow row=null;
{
      lastOffsetPartitions=Maps.newConcurrentMap();
      lastOffsetPartitions.putAll(lastOffsets);
    }
    @Override public void start() throws Exception {
      nextMessage();
    }
    @Override public boolean advance(){
      if (stop) {
        return false;
      }
      nextMessage();
      return true;
    }
    private void nextMessage(){
      try {
        row=null;
        while (row == null) {
          if (msg != null) {
            lastOffsetPartitions.put(msg.getPartition(),msg.offset());
          }
          msg=messageQueue.take();
          interrupted=false;
          final byte[] message=msg.message();
          row=message == null ? null : theParser.parse(ByteBuffer.wrap(message));
        }
      }
 catch (      InterruptedException e) {
        interrupted=true;
        log.info(e,"Interrupted when taken from queue");
      }
    }
    @Override public InputRow currRow(){
      if (interrupted) {
        return null;
      }
      return row;
    }
    @Override public Committer makeCommitter(){
      final Map<Integer,Long> offsets=Maps.newHashMap(lastOffsetPartitions);
      return new Committer(){
        @Override public Object getMetadata(){
          return offsets;
        }
        @Override public void run(){
        }
      }
;
    }
    @Override public void close() throws IOException {
      log.info("Stopping kafka 0.8 simple firehose");
      stop=true;
      for (      PartitionConsumerWorker t : consumerWorkers) {
        t.close();
      }
    }
  }
;
}
