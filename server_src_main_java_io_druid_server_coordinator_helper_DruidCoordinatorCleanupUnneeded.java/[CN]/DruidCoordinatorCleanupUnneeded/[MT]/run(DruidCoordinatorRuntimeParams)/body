{
  CoordinatorStats stats=new CoordinatorStats();
  Set<DataSegment> availableSegments=params.getAvailableSegments();
  DruidCluster cluster=params.getDruidCluster();
  if (!availableSegments.isEmpty()) {
    for (    MinMaxPriorityQueue<ServerHolder> serverHolders : cluster.getSortedServersByTier()) {
      for (      ServerHolder serverHolder : serverHolders) {
        ImmutableDruidServer server=serverHolder.getServer();
        for (        ImmutableDruidDataSource dataSource : server.getDataSources()) {
          for (          DataSegment segment : dataSource.getSegments()) {
            if (!availableSegments.contains(segment)) {
              LoadQueuePeon queuePeon=params.getLoadManagementPeons().get(server.getName());
              if (!queuePeon.getSegmentsToDrop().contains(segment)) {
                queuePeon.dropSegment(segment,new LoadPeonCallback(){
                  @Override public void execute(){
                  }
                }
);
                stats.addToTieredStat("unneededCount",server.getTier(),1);
              }
            }
          }
        }
      }
    }
  }
 else {
    log.info("Found 0 availableSegments, skipping the cleanup of segments from historicals. This is done to prevent a race condition in which the coordinator would drop all segments if it started running cleanup before it finished polling the metadata storage for available segments for the first time.");
  }
  return params.buildFromExisting().withCoordinatorStats(stats).build();
}
