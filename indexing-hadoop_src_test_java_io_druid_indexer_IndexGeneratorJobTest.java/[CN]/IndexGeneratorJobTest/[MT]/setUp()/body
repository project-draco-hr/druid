{
  mapper=HadoopDruidIndexerConfig.JSON_MAPPER;
  mapper.registerSubtypes(new NamedType(HashBasedNumberedShardSpec.class,"hashed"));
  mapper.registerSubtypes(new NamedType(SingleDimensionShardSpec.class,"single"));
  dataFile=temporaryFolder.newFile();
  tmpDir=temporaryFolder.newFolder();
  HashMap<String,Object> inputSpec=new HashMap<String,Object>();
  inputSpec.put("paths",dataFile.getCanonicalPath());
  inputSpec.put("type","static");
  if (inputFormatName != null) {
    inputSpec.put("inputFormat",inputFormatName);
  }
  if (SequenceFileInputFormat.class.getName().equals(inputFormatName)) {
    writeDataToLocalSequenceFile(dataFile,data);
  }
 else {
    FileUtils.writeLines(dataFile,data);
  }
  config=new HadoopDruidIndexerConfig(new HadoopIngestionSpec(new DataSchema("website",mapper.convertValue(inputRowParser,Map.class),new AggregatorFactory[]{new LongSumAggregatorFactory("visited_num","visited_num"),new HyperUniquesAggregatorFactory("unique_hosts","host")},new UniformGranularitySpec(Granularity.DAY,QueryGranularity.NONE,ImmutableList.of(this.interval)),mapper),new HadoopIOConfig(ImmutableMap.copyOf(inputSpec),null,tmpDir.getCanonicalPath()),new HadoopTuningConfig(tmpDir.getCanonicalPath(),null,null,null,null,null,false,false,false,false,ImmutableMap.of(JobContext.NUM_REDUCES,"0"),false,useCombiner)));
  config.setShardSpecs(loadShardSpecs(partitionType,shardInfoForEachSegment));
  config=HadoopDruidIndexerConfig.fromSpec(config.getSchema());
}
